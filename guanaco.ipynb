{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "OpenAssistant-Guanaco is a project focused on developing and fine-tuning chatbot models using a dataset of multilingual human-written simulated conversations. In these conversations, individuals interact with a chatbot assistant, with the twist that the responses from the assistant were authored by real people through crowdsourcing. This dataset offers a diverse range of conversational scenarios, allowing for the training and refinement of chatbot models in various languages and contexts. The primary task associated with this project involves fine-tuning chatbot models using this dataset to enhance their conversational capabilities and overall performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team Members\n",
        "\n",
        "- Balice Matteo\n",
        "\n",
        "- Doronzo Antonio Giuseppe\n",
        "\n",
        "- Fabris Filip\n",
        "\n",
        "- Masini Alessandro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Index\n",
        "\n",
        "2. [Analysis](#analysis)\n",
        "    - [General analysis](##general-analysis)\n",
        "    - [Word2Vec](##word2vec)\n",
        "    - [Clustering](##clustering)\n",
        "\n",
        "3. [Training (fine tuning)](#training)\n",
        "4. [Evaluation](#training)\n",
        "5. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlZFHdKIVOD"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glFpk9eeIeC6"
      },
      "source": [
        "#### Install additional packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install --upgrade gensim\n",
        "!pip install langdetect\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-ireH0YIjZQ"
      },
      "source": [
        "#### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DAlNGNXKIhw1",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import random, re\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from langdetect import detect\n",
        "from pandas.core.common import flatten\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUHZTDBtIyjp"
      },
      "source": [
        "#### Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "da743f96a8644088b9cc3e148f00ae42",
            "dc42789b89b94cc1b84f2a6569c769fd",
            "dffd6a7e61fe412caa6c46712f75f417",
            "20742a5208f84357ad523caeab7bce11",
            "08d2090f14cf46878ed8daeec54af132",
            "669dd15ce9d84be0b52690ae1c939ee1",
            "778757ae1d834c8381c0248a6a353e6e",
            "7e86c22e69054684bb882f2c8d0cab2b",
            "25028418d18d40e289dc7abf01fedc99",
            "55c535f122dc47498ffad9dab206dced",
            "5dbbf8c8285b4aefa8b8341a034298f5",
            "f88f8a0035a84ad7a54ca2d7de19ede4",
            "7f19b6f44b88420fb796d821b7aa2ea1",
            "368b293ca59f4a769b2709621d7ae2a0",
            "e501ec8bfb59446babe057633ef46040",
            "e76d3770a00640dd8703b360dae1d88c",
            "b7d8a51e0a004acfacada7db8c0330ae",
            "48e98e43a19842c19726a5a78008597d",
            "71c089c0388f4cd5999dd2cd3e80ec27",
            "e4b978df62cd49dca6f901269fc2440c",
            "34352f8edc734177a4dd5aa65e923f8e",
            "fd6e9354adff4702a5d3dce8a0d1c96c",
            "bf24977c534e4eb5b275dc9020a285b6",
            "47b4cdf77ba84c62bdd65059a70372e4",
            "8c73089943ae449784a89d444e9b7f1e",
            "a9b3b72a45c44cd59ebd4889426ae53a",
            "02e84784f9764550b000933dbc70c558",
            "ce6754f496904d22860be6525f77e6b1",
            "f00bd38fd36b48fd962f0ca3c6666675",
            "fb99ed37249e4dd7b065cda8fc371551",
            "a196791aa71f4645a2f15d096142f6ef",
            "d4c90b9a31c84c79a3897d141aab6227",
            "8f047cc89e32422dbde213f54743f401",
            "0cb82cd000a34ecaa4a24b7639bedc80",
            "877812f447fa426994de88b237dafd80",
            "e19c27f417694f15a69b2b6987d0a471",
            "5450df6b1e2448d4a587fa828f9b9d5c",
            "3f0e24d7f61c428dac396387bd9b8e1a",
            "b1731f17a6be460b9b2d436eba6ea5d9",
            "e53bed3fc01b42b6b3593d53bed78eb2",
            "3d3fc9747b1e4c7690ab770a9243ec73",
            "77950b7b37f144549315a8bdb763b603",
            "b78a508ef3a440dc9ab715095dabbf1a",
            "a8f7e29d03c74ff8a1dbd0eaa4be2b4a",
            "24ef5fc62f4f49398f60aa54b79b473b",
            "290379391e8a4488a637a759ef120052",
            "d497ff17d92243b39eed6b178f63d8f6",
            "b021ef90c2064ff4841c174a5321ed3d",
            "d8803f3537ab42a4809cfca46ad890cd",
            "13cb26101a5746e19cfd0085ac811c8c",
            "2441716ab068433dba54017a5d53c3af",
            "f888d236be5344d1aa278505be9489a5",
            "c7957c6388184c67aa30a8e47b13dc80",
            "e94d3cef53b14c54a4e4be57fc75aa4d",
            "530210ca296d477c8c75aed4536ac1af"
          ]
        },
        "id": "cq3yaglEImk2",
        "metadata": {},
        "outputId": "8ae775b5-2b43-4bdb-9a2a-6971bba57f0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyCym3wI1PW"
      },
      "source": [
        "#### Analysis of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnloF6xlIpLM",
        "outputId": "21856a89-d64b-4336-b4a5-4b647b4c73c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Structure:\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 9846\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 518\n",
            "    })\n",
            "})\n",
            "\n",
            "Available Splits: dict_keys(['train', 'test'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset Structure:\\n\", dataset)\n",
        "print(\"\\nAvailable Splits:\", dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame(dataset['train'], columns=['text'])\n",
        "test_df = pd.DataFrame(dataset['test'], columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPH7aNJMI48y",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "df = pd.concat([train_df, test_df], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftk-uKTZJXLz",
        "outputId": "64d871d9-ea2f-45fe-fa7d-254ef1c9fdb4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nFirst few entries of the dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example of Human - Assistant Interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.iloc[0]['text'].split(\"###\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Sentence length analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "BhG3gK1YJbAv",
        "outputId": "3662c137-1369-4e71-eefc-f67b84318fd7"
      },
      "outputs": [],
      "source": [
        "df['token_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['token_length'], bins=30, kde=True)\n",
        "plt.title('Sentence Length Distribution')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_length = df['token_length'].min()\n",
        "max_length = df['token_length'].max()\n",
        "avg_length = df['token_length'].mean()\n",
        "\n",
        "print(f\"Minimum Length: {min_length}\")\n",
        "print(f\"Maximum Length: {max_length}\")\n",
        "print(f\"Average Length: {avg_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Search Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyterrier as pt\n",
        "\n",
        "if not pt.started():\n",
        "    pt.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"docno\"] = [str(i) for i in range(1, len(df) + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indexer = pt.DFIndexer(\n",
        "    \"c:\\\\Users\\\\filip\\\\Desktop\\\\NLP-project\\\\local\\\\index\", overwrite=True\n",
        ")\n",
        "index_ref = indexer.index(df[\"text\"], df[\"docno\"])\n",
        "index_ref.toString()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(index.getCollectionStatistics().toString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"human\"\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
        "bm25.search(query).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
        "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"human\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = ((tf % 10) | (tf_idf % 10)) >> bm25\n",
        "\n",
        "pipeline.search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(df[\"text\"])\n",
        "len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Download stop words from NLTK for the languages you are interested in.\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stop_words = set(stopwords.words(\"english\"))\n",
        "spanish_stop_words = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "# Combine the stop words sets.\n",
        "combined_stop_words = english_stop_words.union(spanish_stop_words)\n",
        "combined_stop_words = list(combined_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=8, stop_words=combined_stop_words)\n",
        "X = vectorizer.fit_transform(df[\"text\"])\n",
        "len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Sum up the counts of each term in all documents\n",
        "sums = X.sum(axis=0)  # Sum over columns to get total counts for each feature\n",
        "\n",
        "# Connecting term names with their sums\n",
        "freq = [(word, sums[0, idx]) for word, idx in zip(feature_names, range(sums.shape[1]))]\n",
        "# Sorting the list of tuples by frequency\n",
        "sorted_freq = sorted(freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Creating DataFrame from the sorted frequencies for easier plotting\n",
        "freq_df = pd.DataFrame(sorted_freq, columns=[\"term\", \"frequency\"])\n",
        "\n",
        "# Plotting the top N most frequent terms\n",
        "top_n = 20\n",
        "plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "sns.barplot(data=freq_df.head(top_n), x=\"frequency\", y=\"term\", palette=\"viridis\")\n",
        "plt.title(\"Top 20 Most Frequent Terms\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Terms\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make another plot with the most frequent terms that most frequently appear like bubble chart, use data which you have\n",
        "\n",
        "# Plotting the top N most frequent terms\n",
        "\n",
        "plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "\n",
        "sns.scatterplot(data=freq_df.head(top_n), x=\"frequency\", y=\"term\", size=\"frequency\", sizes=(100, 1000), palette=\"viridis\")\n",
        "plt.title(\"Top 20 Most Frequent Terms\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Terms\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with ### Human: Generate me an ASCII of a person with a jetpack### Assistant: Here is an ASCII art representation of a person with a jetpack:\n",
            "\n",
            "─────────▄███████████▄▄──────────────\n",
            "──────▄██▀──────────▀▀██▄────────────\n",
            "────▄█▀────────────────▀██───────────\n",
            "──▄█▀────────────────────▀█▄─────────\n",
            "─█▀──██──────────────██───▀██────────\n",
            "█▀──────────────────────────██───────\n",
            "█──███████████████████───────█───────\n",
            "█────────────────────────────█───────\n",
            "█────────────────────────────█───────\n",
            "█────────────────────────────█───────\n",
            "█────────────────────────────█───────\n",
            "█────────────────────────────█───────\n",
            "█▄───────────────────────────█───────\n",
            "▀█▄─────────────────────────██───────\n",
            "─▀█▄───────────────────────██────────\n",
            "──▀█▄────────────────────▄█▀─────────\n",
            "───▀█▄──────────────────██───────────\n",
            "─────▀█▄──────────────▄█▀────────────\n",
            "───────▀█▄▄▄──────▄▄▄███████▄▄───────\n",
            "────────███████████████───▀██████▄───\n",
            "─────▄███▀▀────────▀███▄──────█─███──\n",
            "───▄███▄─────▄▄▄▄────███────▄▄████▀──\n",
            "─▄███▓▓█─────█▓▓█───████████████▀────\n",
            "─▀▀██▀▀▀▀▀▀▀▀▀▀███████████────█──────\n",
            "────█─▄▄▄▄▄▄▄▄█▀█▓▓─────██────█──────\n",
            "────█─█───────█─█─▓▓────██────█──────\n",
            "────█▄█───────█▄█──▓▓▓▓▓███▄▄▄█──────\n",
            "────────────────────────██──────────\n",
            "────────────────────────██───▄███▄───\n",
            "────────────────────────██─▄██▓▓▓██──\n",
            "───────────────▄██████████─█▓▓▓█▓▓██▄\n",
            "─────────────▄██▀───▀▀███──█▓▓▓██▓▓▓█\n",
            "─▄███████▄──███───▄▄████───██▓▓████▓█\n",
            "▄██▀──▀▀█████████████▀▀─────██▓▓▓▓███\n",
            "██▀─────────██──────────────██▓██▓███\n",
            "██──────────███──────────────█████─██\n",
            "██───────────███──────────────█─██──█\n",
            "██────────────██─────────────────█───\n",
            "██─────────────██────────────────────\n",
            "██─────────────███───────────────────\n",
            "██──────────────███▄▄────────────────\n",
            "███──────────────▀▀███───────────────\n",
            "─███─────────────────────────────────\n",
            "──███────────────────────────────────### Human: Wow, that is great. Can you give the person give a hat and a beard?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog',\n",
              " '### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in.\\n\\nThe point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart.\\n\\nThis method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.### Human: I didn\\'t understand how pulling and pushing works. What are we pulling or pushing? And how does it work in terms of code or maths?### Assistant: It has to do with a process called differentiation, which means finding a derivative: a function that tells us whether another function is going up or down at a given point. For example, the derivative of `f(x) = x` is `f\\'(x) = 1`, because it\\'s always going up at a 1:1 ratio. This can be done for a variety of functions; notably, if you know the derivatives of `f(x)` and `g(x)`, you can also get the derivative of `f(g(x))` using a formula called the chain rule. Neural networks happen to be made of differentiable functions, so we can take the derivative of parts or all of it.\\n\\nTo use this for \"pushing\" and \"pulling\", we\\'ll put two images through the neural network. Let\\'s say the images are of a cat and a dog, so we want to increase the distance between the two. We pick one neuron weight from the network and make it a variable `x`, then construct a function that calculates the output of the network based on it and all the other parameters; let\\'s call it `N(x)`. The distance between the cat and dog outputs would be `f(x) = N(cat) - N(dog)`. (Of course, the real output would have more than 1 dimension, but we\\'re simplifying.) We now want to nudge the weight such that it moves the two outputs slightly further apart. For that, we can simply take the derivative! If `f\\'(x)` is positive, that means that increasing the weight will move them further apart, so we should do that. If it\\'s negative, then it\\'ll move them closer, so we\\'ll want to slightly decrease the weight instead. Apply this to all the neurons enough times and your network will soon converge to a pretty good cat-dog separator!',\n",
              " \"### Human: I want to start doing astrophotography as a hobby, any suggestions what could i do?### Assistant: Getting started in astrophotography can seem daunting, but with some patience and practice, you can become a master of the craft. To begin, you'll need a good camera and lens, a tripod, and a dark sky location free of light pollution. You will also need to learn about the basics of astrophotography, such as what camera settings to use, how to capture star trails, and the best techniques for tracking celestial objects. You can also purchase or rent different types of telescopes, depending on what types of objects you want to capture. Additionally, it's important to keep up with the latest astrophotography news and trends. Once you have the necessary equipment and knowledge, you can start shooting and experimenting with different techniques to get the best results.### Human: Can you tell me more?  What would you recommend as a basic set of equipment to get started with?  How much will it cost?### Assistant: Astrophotography can be a fun and rewarding hobby, and here are some more in depth suggestions for you to get started:\\n\\n    Equipment: As a beginner, you will need a camera that is capable of taking long exposures and manual control over the settings. A good starting point would be a digital SLR (DSLR) camera or a mirrorless camera. You will also need a sturdy tripod, a fast wide-angle lens (f/2.8 or faster), and a remote shutter release cable to minimize camera shake during long exposures.\\n\\n    Location: Look for a location with minimal light pollution and a clear view of the night sky. Check online maps to find the best spots near you.\\n\\n    Settings: Start with a wide-angle lens and set your camera to manual mode. Set your aperture to f/2.8 or wider, ISO to 800 or 1600, and experiment with different exposure times (30 seconds to 2 minutes).\\n\\n    Focus: Focus the lens on the stars using live view and manual focus. You may also need to use a Bahtinov mask to achieve accurate focus.\\n\\n    Editing: After you've taken your photos, use editing software such as Adobe Lightroom or Photoshop to adjust the exposure, contrast, and color balance to get the best results.\\n\\nThe cost of equipment will depend on the type of camera and lens you choose, but a basic setup can cost anywhere from $500 to $3000. However, you don't need to start with the most expensive equipment to get good results. As you improve, you can upgrade your equipment and add additional accessories such as filters and tracking mounts.\\n\\nRemember, astrophotography is a hobby that requires patience and practice, so don't be discouraged if your first few attempts don't turn out as well as you had hoped. Keep experimenting and learning, and you'll soon be able to capture stunning images of the night sky!### Human: What is a good course of instruction to avoid years of wasted effort?\",\n",
              " '### Human: Can you give me an example of a python script that opens an api point and serves a string?### Assistant: Sure! Here\\'s an example Python script that uses the Flask web framework to create a simple API endpoint that serves a string:\\n\\n``` \\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/\\')\\ndef hello_world():\\n    return \\'Hello, world!\\'\\n\\nif __name__ == \\'__main__\\':\\n    app.run()\\n\\n``` \\n\\nIn this script, we first import the Flask class from the flask module. Then we create a new instance of the Flask class, using the __name__ variable to specify the name of the application.\\n\\\\\\nNext, we define a new route using the @app.route() decorator. This decorator tells Flask to map requests to the root URL (\"/\") to the hello_world() function.\\n\\\\\\nFinally, we use the if __name__ == \\'__main__\\': block to start the Flask application when the script is executed. By default, the application will run on port 5000.\\n\\\\\\nYou can run this script and test the API by opening a web browser and navigating to http://localhost:5000/. You should see a simple \"Hello, world!\" message displayed in your browser.### Human: What changes would you need to make to the code above to serve a JSON object instead of a string?### Assistant: To serve a JSON object instead of a string, you can modify the \"hello_world()\" function to return a JSON response using the Flask \"jsonify\" function. Here\\'s an example of how to modify the previous code to serve a JSON object:\\n\\n```\\nfrom flask import Flask, jsonify\\n\\napp = Flask(name)\\n\\n@app.route(\\'/\\')\\ndef hello_world():\\ndata = {\\n\\'message\\': \\'Hello, world!\\'\\n}\\nreturn jsonify(data)\\n\\nif name == \\'main\\':\\napp.run()\\n```\\n\\nIn this updated version of the code, we import the \"jsonify\" function from Flask and use it to create a JSON response. We define a dictionary called \"data\" that contains a single key-value pair, where the key is \"message\" and the value is \"Hello, world!\".\\n\\nThe \"jsonify\" function takes a dictionary as its argument and returns a JSON-formatted response with the appropriate content-type headers. When we visit the root URL of our Flask application, we will see a JSON object with the \"message\" key and its associated value.\\n\\nYou can test this modified script in the same way as the previous example, by opening a web browser and navigating to http://localhost:5000/. This time, you should see a JSON object with the \"message\" key and its associated value displayed in your browser.',\n",
              " \"### Human: How can I learn to optimize my webpage for search engines?### Assistant: Learning to optimize your webpage for search engines involves several steps, including:\\n\\n1. Understanding the basics of how search engines work: Learn how search engines crawl and index web pages, and how they rank pages based on factors like relevance, authority, and user experience.\\n2. Conducting keyword research: Identify the keywords and phrases that your target audience is using to search for your products or services, and use these keywords to optimize your web pages.\\n3. Creating high-quality content: Create content that is informative, engaging, and relevant to your target audience. This can include blog posts, articles, videos, and other types of content.\\n4. Optimizing your web pages: Optimize your page titles, meta descriptions, header tags, and other on-page elements to make sure they are aligned with your target keywords and provide a clear picture of what your page is about.\\n5. Building backlinks: Build high-quality backlinks from other reputable websites to your own site. This can help increase your authority and improve your search engine rankings.\\n6. Monitoring your results: Use analytics tools to track your website's performance in search engine rankings, and adjust your strategy as needed based on your results.\\n\\nThere are many resources available online to help you learn more about search engine optimization (SEO), including blogs, courses, and tutorials. Some popular SEO resources include Moz, SEMrush, and Google's Search Console documentation.\\n\\nIt's important to note that SEO is an ongoing process, and it can take time and effort to see results. However, by following best practices and staying up-to-date with the latest trends and techniques, you can help improve your website's visibility and attract more traffic from search engines.### Human: My webpage is about flower arrangements. Can you give some examples for each of the above?\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = dataset['train']['text'] + dataset['test']['text']\n",
        "\n",
        "def filter_english(strings):\n",
        "  english_strings = []\n",
        "  for string in strings:\n",
        "    try:\n",
        "      if detect(string) == 'en':\n",
        "        english_strings.append(string)\n",
        "    except:\n",
        "      print(f\"Error with {string}\")\n",
        "  return english_strings\n",
        "\n",
        "docs = filter_english(docs)\n",
        "docs[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's separate the sentences written by the human from the ones of the assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5485\n",
            "5485\n"
          ]
        }
      ],
      "source": [
        "human_questions = []\n",
        "assistant_answers = []\n",
        "\n",
        "for string in docs:\n",
        "  segments = string.split(\"### Human:\")\n",
        "  for segment in segments[1:]:\n",
        "    #This if is added to add to the list of questions only those that have then received an answer\n",
        "    if \"### Assistant:\" in segment:\n",
        "      human_questions.append(segment.split(\"### Assistant:\")[0].strip())\n",
        "\n",
        "  segments = string.split(\"### Assistant:\")\n",
        "  for segment in segments[1:]:\n",
        "      assistant_answers.append(segment.split(\"### Human:\")[0].strip())\n",
        "\n",
        "print(len(human_questions))\n",
        "print(len(assistant_answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's clean the datasets and split them into different sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_questions = [re.sub('[\\n\\t]', '', doc) for doc in human_questions]\n",
        "assistant_answers = [re.sub('[\\n\\t]', '', doc) for doc in assistant_answers]\n",
        "human_questions = [re.sub('[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', doc) for doc in human_questions]\n",
        "assistant_answers = [re.sub('[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', doc) for doc in assistant_answers]\n",
        "human_questions = [re.split('[?!.]\\s', doc) for doc in human_questions]\n",
        "assistant_answers = [re.split('[?!.]\\s', doc) for doc in assistant_answers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Flatten the list of lists and tokenize each word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "human_questions = list(flatten(human_questions))\n",
        "human_questions = [re.sub('\\W', ' ', doc).lower().split() for doc in human_questions]\n",
        "\n",
        "assistant_answers = list(flatten(assistant_answers))\n",
        "assistant_answers = [re.sub('\\W', ' ', doc).lower().split() for doc in assistant_answers]\n",
        "\n",
        "#We also compute a combination of the two tet\n",
        "full_text = human_questions + assistant_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The minimum length of each sentence is 5, so we don't need to remove sentences that are composed by few words\n",
        "\n",
        " Now we train the Word2Vec on the human questions, by providing as input the tokenized words, the size of each embedding, the minimum number of occurences for each word and the context window size:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "model_human = Word2Vec(human_questions, vector_size=30, min_count=5, window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many words do we have in our model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(model_human.wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How is each embedding vector made?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'house'\n",
        "model_human.wv[term]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are the most similar words to the word \"short\" ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'short'\n",
        "model_human.wv.most_similar(term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's do the same thing but with the assistant answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "model_assistant = Word2Vec(assistant_answers, vector_size=30, min_count=5, window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's vocabulary size is more than 4 times the human one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(model_assistant.wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see an example of embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'house'\n",
        "model_assistant.wv[term]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the most similar words to the word \"short\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'short'\n",
        "model_assistant.wv.most_similar(term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's perform the same operations with the full_text to our disposal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_full = Word2Vec(full_text, vector_size=30, min_count=5, window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The length of the vocabulary isn't much different from the one of just the assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(model_full.wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's again an example of an embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'plant'\n",
        "model_full.wv[term]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And once again the most similar word to the word short:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "term = 'short'\n",
        "model_full.wv.most_similar(term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Has the model understood the relation between this words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vec = model_full.wv['king'] + (model_full.wv['woman'] - model_full.wv['man'])\n",
        "vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seems it does not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_full.wv.most_similar(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And this kind of relation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vec = model_full.wv['france'] + (model_full.wv['rome'] - model_full.wv['italy'])\n",
        "vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems to have a better idea of what we are talking about, but still he hasn't fully understood the relation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_full.wv.most_similar(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Visualizing the embedding vector using t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll now produce a graphical representation of a subset of the embeddings, because to reduce the time required for computation, we'll limit our representation to 500 random samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_samples_human = random.sample(list(model_human.wv.key_to_index), 500)\n",
        "word_vectors_human = model_human.wv[random_samples_human]\n",
        "\n",
        "random_samples_assistant = random.sample(list(model_assistant.wv.key_to_index), 500)\n",
        "word_vectors_assistant = model_assistant.wv[random_samples_assistant]\n",
        "\n",
        "random_samples_full = random.sample(list(model_full.wv.key_to_index), 500)\n",
        "word_vectors_full = model_full.wv[random_samples_full]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll provide the vectors to the TSNE algorithm, to fit a model and have a 3 dimensional representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "\n",
        "tsne_embeddings_human = tsne.fit_transform(word_vectors_human)\n",
        "tsne_embeddings_assistant = tsne.fit_transform(word_vectors_assistant)\n",
        "tsne_embeddings_full = tsne.fit_transform(word_vectors_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We transpose the matrix, as to have each dimension in each row of the resulting matrix and we get the coordinates of each point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_human, y_human, z_human = np.transpose(tsne_embeddings_human)\n",
        "x_assistant, y_assistant, z_assistant = np.transpose(tsne_embeddings_assistant)\n",
        "x_full, y_full, z_full = np.transpose(tsne_embeddings_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We draw the plots (we also reduced the amount of samples to be shown for a better representation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(x=x_human[:150], y=y_human[:150], z=z_human[:150], text=random_samples_human[:150])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(x=x_assistant[:150], y=y_assistant[:150], z=z_assistant[:150], text=random_samples_assistant[:150])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(x=x_full[:150], y=y_full[:150], z=z_full[:150], text=random_samples_full[:150])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Automatic clustering based on the embeddings of the word2vec model.\n",
        "We put a threshold of 0.95 to have a good clustering.\n",
        "\n",
        "We use the cosine similarity to calculate the distance between the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['salt', 'pepper']\n",
            "Cluster 1:\n",
            "['landroid', 'hidl', 'ljava', 'throws', 'remoteexception', 'v1_0']\n",
            "Cluster 2:\n",
            "['garlic', 'onion']\n",
            "Cluster 3:\n",
            "['annotation', 'ldalvik']\n",
            "Cluster 4:\n",
            "['rick', 'morty']\n",
            "Cluster 5:\n",
            "['bin', 'usr']\n",
            "Cluster 6:\n",
            "['beans', 'chopped']\n",
            "Cluster 7:\n",
            "['tk_calc', 'sticky', 'button_click', 'nsew', 'button_params']\n",
            "Cluster 8:\n",
            "['ich', 'und']\n",
            "Cluster 9:\n",
            "['v0', 'invoke']\n",
            "Cluster 10:\n",
            "['con', 'una', 'código']\n",
            "Cluster 11:\n",
            "['diced', 'tbsp']\n",
            "Cluster 12:\n",
            "['dlp', 'yt']\n",
            "Cluster 13:\n",
            "['file_name', 'local_file']\n",
            "Cluster 14:\n",
            "['para', 'interfaz']\n",
            "Cluster 15:\n",
            "['idx', 'ht']\n",
            "Cluster 16:\n",
            "['x1', 'x2', 'y1']\n",
            "Cluster 17:\n",
            "['columna', 'fila']\n",
            "Cluster 18:\n",
            "['latte', 'caffe']\n",
            "Cluster 19:\n",
            "['120', '65']\n",
            "Cluster 20:\n",
            "['minced', 'tablespoon']\n",
            "Cluster 21:\n",
            "['ihwbinder', 'p0']\n",
            "Cluster 22:\n",
            "['fixed_type', 'stream_type']\n",
            "Cluster 23:\n",
            "['num1', 'num2']\n",
            "Cluster 24:\n",
            "['padx', 'pady']\n",
            "Cluster 25:\n",
            "['mai', '4096']\n",
            "Cluster 26:\n",
            "['oregano', 'basil']\n",
            "Cluster 27:\n",
            "['radius1', 'radius2']\n",
            "Cluster 28:\n",
            "['0100000', '01100101', '01110100', '01110011']\n",
            "Cluster 29:\n",
            "['hidden_size', 'input_size', 'output_size']\n",
            "Cluster 30:\n",
            "['abdul', 'jabbarlos', 'jabbarmilwaukee']\n",
            "Cluster 31:\n",
            "['getlasterror', 'syserrormessage']\n",
            "Cluster 32:\n",
            "['intlist', 'strlist']\n",
            "Cluster 33:\n",
            "['input_stream', 'output_stream']\n",
            "Cluster 34:\n",
            "['output_folder', 'nf']\n",
            "Cluster 35:\n",
            "['belt', 'kuiper']\n",
            "Cluster 36:\n",
            "['153', '151']\n",
            "Cluster 37:\n",
            "['jordanchicago', 'johnsonlos']\n",
            "Cluster 38:\n",
            "['sans', 'serif']\n",
            "Cluster 39:\n",
            "['costa', 'rica']\n",
            "Cluster 40:\n",
            "['157', '147']\n",
            "Cluster 41:\n",
            "['paddle_height', 'paddle_width']\n",
            "Cluster 42:\n",
            "['attention_mask', 'input_ids']\n",
            "Cluster 43:\n",
            "['weeklys', 'nov']\n",
            "Cluster 44:\n",
            "['154', '152', '156']\n",
            "Cluster 45:\n",
            "['420', 'patty']\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity_matrix(embeddings):\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)\n",
        "    return similarity_matrix\n",
        "\n",
        "def find_similar_groups(embeddings, threshold):\n",
        "    similarity_matrix = cosine_similarity_matrix(embeddings)\n",
        "    n = len(embeddings)\n",
        "    visited = set()\n",
        "    similar_groups = []\n",
        "\n",
        "    for i in range(n):\n",
        "        if i not in visited:\n",
        "            similar_group = [i]\n",
        "            visited.add(i)\n",
        "            for j in range(i+1, n):\n",
        "                if j not in visited and similarity_matrix[i][j] >= threshold:\n",
        "                    similar_group.append(j)\n",
        "                    visited.add(j)\n",
        "            similar_groups.append(similar_group)\n",
        "\n",
        "    return similar_groups\n",
        "\n",
        "threshold = 0.99\n",
        "embeddings_word2vec = model_full.wv.vectors\n",
        "similar_groups = find_similar_groups(embeddings_word2vec, threshold)\n",
        "\n",
        "i = 0\n",
        "for _, group in enumerate(similar_groups):\n",
        "    if len(group) == 1:\n",
        "        continue\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print([model_full.wv.index_to_key[idx] for idx in group])\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clustering based on the languages of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_languages = df['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
        "vectorizer.fit(documents_languages)\n",
        "vector_documents = vectorizer.transform(documents_languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"Length of vocabulary: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Number of clusters (languages)\n",
        "k = 5\n",
        "\n",
        "kmeans = KMeans(n_clusters=k, max_iter=100, n_init=2, verbose=True, random_state=2307)\n",
        "kmeans.fit(vector_documents)\n",
        "labels = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx in range(k):\n",
        "  centroid = kmeans.cluster_centers_[idx]\n",
        "\n",
        "  # Sort terms according to their weights \n",
        "  # (argsort goes from lowest to highest, we reverse the order through slicing)\n",
        "  sorted_terms = centroid.argsort()[::-1]\n",
        "\n",
        "  # Print out the top 10 terms for the cluster\n",
        "  print(\"Cluster \" + str(idx) + \":\")\n",
        "  print([vocab[j] for j in sorted_terms[:20]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(2)\n",
        "reduced_data = svd.fit_transform(vector_documents)\n",
        "\n",
        "[x,y] = np.transpose(reduced_data)\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "sns.scatterplot(x=x, y=y, hue=labels, palette=['red', 'blue', 'green', 'purple', 'orange'])\n",
        "plt.legend(title=\"Cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training (fine tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.21.4)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface_hub) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/matteobalice/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/matteobalice/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /Users/matteobalice/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login, logout\n",
        "login(\"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\")\n",
        "# !huggingface-cli login --token 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig, AutoConfig\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import torch\n",
        "from peft import PeftConfig, PeftModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama3 8B Model Overview\n",
        "\n",
        "The Llama3 8B Model is an advanced large language model with 8 billion parameters, designed for a wide range of natural language processing tasks. This model leverages deep learning techniques to understand and generate human-like text, providing improvements in both accuracy and fluency over its predecessors.\n",
        "\n",
        "**Key Characteristics**:\n",
        "- **Model Size**: 8 billion parameters, enabling complex understanding and generation capabilities.\n",
        "- **Training Data**: Trained on a diverse corpus from books, articles, and websites to ensure broad knowledge and applicability.\n",
        "- **Applications**: Ideal for tasks such as text summarization, question answering, and language translation.\n",
        "\n",
        "The Llama3 8B Model represents a significant step forward in the field of AI-driven text analysis and generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## For 4 bit quantization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, adapter_model_name)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:3165\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3165\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3168\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3169\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
          ]
        }
      ],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "adapter_model_name = \"./Possible models/llama-8-finetuned-smallDataset/checkpoint-150\"\n",
        "\n",
        "## For 4 bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id, \n",
        "  quantization_config=quantization_config,\n",
        "  device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, adapter_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chat function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_model(messages, model, tokenizer, terminators):\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=128,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1] :]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guanaco Dataset Summary\n",
        "\n",
        "The Guanaco Dataset provides comprehensive ecological data for \\textit{Lama guanicoe}, native to South America. This dataset is essential for studying guanaco population dynamics, habitat preferences, and conservation needs.\n",
        "\n",
        "**Key Features**:\n",
        "- **Geographic Coverage**: Includes data from diverse ecosystems like the Patagonian steppes and Andean regions.\n",
        "- **Data Points**: Covers population counts, migration patterns, and genetic diversity.\n",
        "- **Applications**: Useful for ecological research and informing conservation policies.\n",
        "\n",
        "This concise dataset description is ideal for supporting research focused on the sustainability and protection of guanacos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "human_questions = []\n",
        "human_answer = []\n",
        "\n",
        "for index in range(50):\n",
        "    string = dataset_train[\"text\"][index]\n",
        "    segments = string.split(\"### Human:\")\n",
        "    for segment in segments[1:]:\n",
        "        # This if is added to add to the list of questions only those that have then received an answer\n",
        "        if \"### Assistant:\" in segment:\n",
        "            human_questions.append(segment.split(\"### Assistant:\")[0].strip())\n",
        "\n",
        "    segments = string.split(\"### Assistant:\")\n",
        "    for segment in segments[1:]:\n",
        "        human_answer.append(segment.split(\"### Human:\")[0].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe with questions and answers\n",
        "import pandas as pd\n",
        "\n",
        "df_qa = pd.DataFrame(\n",
        "    list(zip(human_questions, human_answer)), columns=[\"question\", \"human_answer\"]\n",
        ")\n",
        "\n",
        "df_qa = df_qa.head(50)\n",
        "\n",
        "# Dictionary with questions and answers where index is the key\n",
        "qa_dict = df_qa.to_dict(orient=\"index\")\n",
        "qa_dict[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answers(model, df_qa):\n",
        "    assistant_answers = []\n",
        "\n",
        "    for index in range(50):\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Normaly answer to the question\"\n",
        "            },\n",
        "\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": df_qa[\"question\"][index]\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        response = chat_with_model(messages, model, tokenizer, terminators)\n",
        "        assistant_answers.append(response)\n",
        "\n",
        "assistant_answers = generate_answers(model, df_qa)\n",
        "df_qa[\"assistant_answer\"] = assistant_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Metrics for Evaluating Large Language Models (LLMs)\n",
        "\n",
        "Evaluating Large Language Models involves several metrics that capture different aspects of model performance. Here are some commonly used metrics:\n",
        "\n",
        "- **Perplexity (PPL)**: Measures how well a probability model predicts a sample. A lower perplexity indicates the model predicts the sample better, often used for language models.\n",
        "\n",
        "$$\n",
        "\\text{PPL} = 2^{-\\frac{1}{N} \\sum_{i=1}^N \\log_2 p(x_i)}\n",
        "$$\n",
        "\n",
        "Where $N$ is the length of the text and $p(x_i)$ is the probability of the word $x_i$.\n",
        "\n",
        "- **BLEU Score**: Evaluates the quality of text which has been machine translated from one language to another by measuring the precision of n-grams between the machine output and reference translations.\n",
        "\n",
        "- **ROUGE Score**: Focused on recall, ROUGE evaluates how many words from the reference summaries appear in the machine-generated summaries.\n",
        "\n",
        "- **METEOR Score**: Aligns words semantically based on exact, stem, synonym, and paraphrase matches between the machine-generated text and reference texts, and computes a score considering both precision and recall.\n",
        "\n",
        "- **F1 Score**: The harmonic mean of precision and recall, commonly used in classification tasks within NLP.\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "These metrics help in assessing different facets of language understanding, generation, and translation capabilities of LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distance Embedding Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the similarity between the embeddings of predicted and true sentences\n",
        "def embedding_similarity():\n",
        "\n",
        "    if not model_full or not hasattr(model_full, \"vw\"):\n",
        "        print(\"Run the word2vec code before so as to generate the embedding!\")\n",
        "        return\n",
        "    \n",
        "    sentences_prediction = df_qa[\"assistant_answer\"]\n",
        "    sentences_true = df_qa[\"human_answer\"]\n",
        "\n",
        "    # Tokenization\n",
        "    tokens_prediction = [sentence.split() for sentence in sentences_prediction]\n",
        "    tokens_true = [sentence.split() for sentence in sentences_true]\n",
        "\n",
        "    embedding_prediction = [np.mean([model_full.vw[word] for word in tokens_prediction[i] if word in model_full.vw], axis=0) for i in range(len(tokens_prediction))]\n",
        "    embedding_true = [np.mean([model_full.vw[word] for word in tokens_true[i] if word in model_full.vw], axis=0) for i in range(len(tokens_true))]\n",
        "\n",
        "    embedding_prediction = np.array(embedding_prediction)\n",
        "    embedding_true = np.array(embedding_true)\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(embedding_prediction)):\n",
        "        similarity = np.dot(embedding_prediction[i], embedding_true[i]) / (np.linalg.norm(embedding_prediction[i]) * np.linalg.norm(embedding_true[i]))\n",
        "\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    return np.mean(similarities)\n",
        "\n",
        "similarity = embedding_similarity()\n",
        "print(\"Embedding Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Perplexity (PPL)\n",
        "\n",
        "Perplexity is a measure of how well a probability model predicts a sample. It is commonly used in natural language processing for evaluating language models. A lower perplexity indicates that the model is better at predicting the sample.\n",
        "\n",
        "For a given text sequence $x_1, x_2, ..., x_N$, the perplexity is calculated as:\n",
        "\n",
        "$$\n",
        "\\text{PPL} = 2^{-\\frac{1}{N} \\sum_{i=1}^N \\log_2 p(x_i)}\n",
        "$$\n",
        "\n",
        "Where $N$ is the length of the text and $p(x_i)$ is the probability assigned by the model to the word $x_i$. Lower perplexity values indicate better model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perplexity(logits, labels):\n",
        "    # Shift the logits and labels to align and calculate loss\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "    # Flatten the tokens\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "def calculate_perplexity():\n",
        "    total_ppl = 0\n",
        "    count = 0\n",
        "    for idx in range(50):\n",
        "    \n",
        "        example = dataset_train[idx][\"text\"]\n",
        "\n",
        "        input = tokenizer(example, return_tensors=\"pt\")\n",
        "        input = input[\"input_ids\"].to(model.device)\n",
        "\n",
        "        labels = input.clone()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            ppl = perplexity(logits, labels)\n",
        "            total_ppl += ppl\n",
        "            count += 1\n",
        "\n",
        "    average_ppl = total_ppl / count\n",
        "    return average_ppl\n",
        "\n",
        "average_ppl = calculate_perplexity()\n",
        "print(f\"Average Perplexity: {average_ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BLEU Score\n",
        "\n",
        "The BLEU (Bilingual Evaluation Understudy) Score is a method for evaluating the quality of text which has been machine translated from one language to another. It is designed to measure the correspondence between a machine's output and that of a human.\n",
        "\n",
        "The formula for BLEU is as follows:\n",
        "\n",
        "$$\n",
        "\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^N w_n \\log p_n\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $p_n$ is the precision of n-grams,\n",
        "- $w_n$ are weights summing to 1,\n",
        "- $BP$ (Brevity Penalty) addresses the translation's length.\n",
        "\n",
        "The Brevity Penalty is calculated as:\n",
        "\n",
        "$$\n",
        "BP = \n",
        "\\begin{cases} \n",
        "1 & \\text{if } c > r \\\\\n",
        "e^{(1-r/c)} & \\text{if } c \\leq r\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- $c$ is the length of the candidate translation,\n",
        "- $r$ is the effective reference corpus length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-47f88e80fceb>:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  bleu = load_metric(\"bleu\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/bleu/bleu.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7915e11ace104c8a996189d731c7d9c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b12e3ed77f6142e59eb59feac67ec143",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: {'bleu': 0.03232040883057138, 'precisions': [0.36705882352941177, 0.13095238095238096, 0.06506024096385542, 0.03414634146341464], 'brevity_penalty': 0.31794325582027044, 'length_ratio': 0.46600877192982454, 'translation_length': 425, 'reference_length': 912}\n"
          ]
        }
      ],
      "source": [
        "def bleu():\n",
        "    # Extract predictions and references from DataFrame\n",
        "    predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "    references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "    # Tokenize predictions\n",
        "    tokenized_predictions = [pred.split() for pred in predictions]\n",
        "\n",
        "    # Tokenize and wrap each reference in a list\n",
        "    tokenized_references = [\n",
        "        [ref.split()] for ref in references\n",
        "    ]  # Wrap each tokenized reference in another list\n",
        "\n",
        "    # Load BLEU metric\n",
        "    bleu = load_metric(\"bleu\")\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = bleu.compute(\n",
        "        predictions=tokenized_predictions, references=tokenized_references\n",
        "    )\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Print the BLEU score\n",
        "bleu_score = bleu()\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ROUGE Score\n",
        "\n",
        "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score is used to evaluate automatic summarization and machine translation software. It compares an automatically produced summary or translation against a set of reference summaries, typically human-generated.\n",
        "\n",
        "The formula for the most commonly used variant, ROUGE-N, is as follows:\n",
        "\n",
        "$$\n",
        "\\text{ROUGE-N} = \\frac{{\\sum_{s \\in \\{Reference Summaries\\}} \\sum_{gram_n \\in s} Count_{match}(gram_n)}}{{\\sum_{s \\in \\{Reference Summaries\\}} \\sum_{gram_n \\in s} Count(gram_n)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $gram_n$ represents n-grams,\n",
        "- $Count_{match}(gram_n)$ is the count of n-grams in both the candidate summary and the reference summary,\n",
        "- $Count(gram_n)$ is the count of n-grams in the reference summary.\n",
        "\n",
        "This formula essentially calculates the proportion of n-grams in the reference summaries that are also found in the candidate summary, highlighting the importance of recall in the evaluation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': AggregateScore(low=Score(precision=0.22573382997165412, recall=0.13577247748075197, fmeasure=0.16377242524916946), mid=Score(precision=0.44865298568650874, recall=0.27928264560350363, fmeasure=0.32671875060011907), high=Score(precision=0.6037262583095917, recall=0.44276239772917014, fmeasure=0.4732187506001191)), 'rouge2': AggregateScore(low=Score(precision=0.05459963472075362, recall=0.028819257777106685, fmeasure=0.03880556705745675), mid=Score(precision=0.16083295370482595, recall=0.09400407731803082, fmeasure=0.11227756532316427), high=Score(precision=0.2654776276910088, recall=0.17353896103896105, fmeasure=0.18673226892278957)), 'rougeL': AggregateScore(low=Score(precision=0.15441937826504556, recall=0.07534372461283988, fmeasure=0.10118989987613544), mid=Score(precision=0.3045227694974691, recall=0.19185218358362768, fmeasure=0.22342858240217717), high=Score(precision=0.4413817663817664, recall=0.3483624584887107, fmeasure=0.37297638822971046)), 'rougeLsum': AggregateScore(low=Score(precision=0.15441937826504554, recall=0.0708913682867358, fmeasure=0.09281312292358804), mid=Score(precision=0.3045227694974691, recall=0.19185218358362768, fmeasure=0.22342858240217717), high=Score(precision=0.4500237416904083, recall=0.34833193570929416, fmeasure=0.3731376456231772))}\n"
          ]
        }
      ],
      "source": [
        "def rouge():\n",
        "    # Extract predictions and references from DataFrame\n",
        "    predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "    references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "    # Tokenize predictions\n",
        "    tokenized_predictions = [pred.split() for pred in predictions]\n",
        "\n",
        "    # Tokenize and wrap each reference in a list\n",
        "    tokenized_references = [\n",
        "        [ref.split()] for ref in references\n",
        "    ]  # Wrap each tokenized reference in another list\n",
        "\n",
        "    # Adjust references for ROUGE\n",
        "    rouge_references = [\n",
        "        ref[0] for ref in tokenized_references\n",
        "    ]\n",
        "\n",
        "    # Load ROUGE metric\n",
        "    rouge = load_metric(\"rouge\")\n",
        "\n",
        "    # Compute ROUGE score\n",
        "    rouge_score = rouge.compute(\n",
        "        predictions=tokenized_predictions, references=rouge_references\n",
        "    )\n",
        "    return rouge_score\n",
        "\n",
        "# Print the ROUGE score\n",
        "rouge_score = rouge()\n",
        "print(\"ROUGE Score:\", rouge_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### METEOR Score\n",
        "\n",
        "The METEOR (Metric for Evaluation of Translation with Explicit ORdering) Score is a metric used to evaluate the quality of translations in natural language processing. Unlike BLEU, METEOR accounts for synonymy and stemming, and it aims to align words between the translated and reference texts more effectively.\n",
        "\n",
        "The basic formula for METEOR is as follows:\n",
        "\n",
        "$$\n",
        "\\text{METEOR} = (1 - Pen) \\cdot F_{mean}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $F_{mean}$ is the harmonic mean of precision and recall,\n",
        "- $Pen$ is a penalty for word order differences.\n",
        "\n",
        "The harmonic mean, $F_{mean}$, is calculated as:\n",
        "\n",
        "$$\n",
        "F_{mean} = \\frac{10 \\cdot P \\cdot R}{R + 9 \\cdot P}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $P$ is precision (the percentage of words in the translation that are correct),\n",
        "- $R$ is recall (the percentage of words in the reference that appear in the translation).\n",
        "\n",
        "The penalty, $Pen$, is calculated based on the number of chunks (contiguous non-matching phrases) in the alignment:\n",
        "\n",
        "$$\n",
        "Pen = 0.5 \\cdot \\left(\\frac{\\text{number of chunks}}{\\text{number of unigrams in the hypothesis}}\\right)^3\n",
        "$$\n",
        "\n",
        "This metric thus combines the lexical accuracy and structural similarity into a single score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/meteor/meteor.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae77d5373e0d49f5a00b9f550de87416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'meteor': 0.2070710147524105}\n"
          ]
        }
      ],
      "source": [
        "def meteor():\n",
        "  \n",
        "    # Extract predictions and references from DataFrame\n",
        "    predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "    references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "    # Tokenize predictions\n",
        "    tokenized_predictions = [pred.split() for pred in predictions]\n",
        "\n",
        "    # Tokenize and wrap each reference in a list\n",
        "    tokenized_references = [\n",
        "        [ref.split()] for ref in references\n",
        "    ]  # Wrap each tokenized reference in another list\n",
        "\n",
        "    joined_predictions = [\" \".join(pred) for pred in tokenized_predictions]\n",
        "    joined_references = [\n",
        "        \" \".join(ref[0]) for ref in tokenized_references\n",
        "    ]  # Assuming only one reference per prediction and removing one level of list nesting\n",
        "\n",
        "    # Load METEOR metric\n",
        "    meteor = load_metric(\"meteor\")\n",
        "\n",
        "    # Compute METEOR score\n",
        "    meteor_score = meteor.compute(\n",
        "        predictions=joined_predictions, references=joined_references\n",
        "    )\n",
        "\n",
        "    return meteor_score\n",
        "\n",
        "# Print the METEOR score\n",
        "meteor_score = meteor()\n",
        "print(\"METEOR Score:\", meteor_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[454, 1331, 1635, 982, 509]\n",
            "982.2\n"
          ]
        }
      ],
      "source": [
        "import Levenshtein\n",
        "\n",
        "def levenshtein():\n",
        "  \n",
        "  predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "  references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "  distances = [Levenshtein.distance(t, p) for t, p in zip(references, predictions)]\n",
        "  final_distance = sum(distances) / len(distances)\n",
        "\n",
        "  return final_distance\n",
        "\n",
        "levenshtein_distance = levenshtein()\n",
        "print(\"Levenshtein Distance:\", levenshtein_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9200043682659775\n"
          ]
        }
      ],
      "source": [
        "import jiwer\n",
        "\n",
        "def wer():\n",
        "    \n",
        "    predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "    references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.Strip(),\n",
        "        jiwer.ExpandCommonEnglishContractions()\n",
        "    ])\n",
        "\n",
        "    errors = []\n",
        "    for true, pred in zip(references, predictions):\n",
        "        transformed_true = transformation(true)\n",
        "        transformed_pred = transformation(pred)\n",
        "        wer_score = jiwer.wer(transformed_true, transformed_pred)\n",
        "        errors.append(wer_score)\n",
        "\n",
        "    average_wer = sum(errors) / len(errors) if errors else 0\n",
        "    return average_wer\n",
        "\n",
        "wer_score = wer()\n",
        "print(\"WER Score:\", wer_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3678937538651776\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(references + predictions)\n",
        "\n",
        "similarities = [cosine_similarity(tfidf[i:i+1], tfidf[len(references)+i:len(predictions)+i+1])[0][0] for i in range(len(references))]\n",
        "similarity = sum(similarities) / len(similarities)\n",
        "\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeat the evaluation with the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "## For 4 bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id, \n",
        "  quantization_config=quantization_config,\n",
        "  device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe with questions and answers\n",
        "import pandas as pd\n",
        "\n",
        "df_qa = pd.DataFrame(\n",
        "    list(zip(human_questions, human_answer)), columns=[\"question\", \"human_answer\"]\n",
        ")\n",
        "\n",
        "df_qa = df_qa.head(50)\n",
        "\n",
        "# Dictionary with questions and answers where index is the key\n",
        "qa_dict = df_qa.to_dict(orient=\"index\")\n",
        "qa_dict[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate answers for the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assistant_answers = generate_answers(model, df_qa)\n",
        "df_qa[\"assistant_answer\"] = assistant_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity = embedding_similarity()\n",
        "print(\"Embedding Similarity:\", similarity)\n",
        "\n",
        "average_ppl = calculate_perplexity(dataset_train)\n",
        "print(f\"Average Perplexity: {average_ppl}\")\n",
        "\n",
        "bleu_score = bleu()\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "\n",
        "rouge_score = rouge()\n",
        "print(\"ROUGE Score:\", rouge_score)\n",
        "\n",
        "meteor_score = meteor()\n",
        "print(\"METEOR Score:\", meteor_score)\n",
        "\n",
        "levenshtein_distance = levenshtein()\n",
        "print(\"Levenshtein Distance:\", levenshtein_distance)\n",
        "\n",
        "wer_score = wer()\n",
        "print(\"WER Score:\", wer_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SQuAD Dataset\n",
        "\n",
        "The Stanford Question Answering Dataset (SQuAD) is a popular dataset in the field of natural language processing used for training and evaluating machine learning models on the task of question answering. Developed by researchers at Stanford University, SQuAD provides a set of questions based on a collection of Wikipedia articles, where the answer to each question is a segment of text (a span) from the corresponding reading passage.\n",
        "\n",
        "The key features of SQuAD are as follows:\n",
        "\n",
        "- **SQuAD 1.1**: Contains over 100,000 question-answer pairs on 500+ articles, where the answers are always exact spans of text from the passages.\n",
        "\n",
        "- **SQuAD 2.0**: Builds upon the previous version by adding over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. This version tests the model's ability not only to find correct answers but also to predict when no answer is supported by the text.\n",
        "\n",
        "Both versions are designed to mimic the process of human reading comprehension, making SQuAD a challenging and influential dataset in the AI community.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_sq = load_dataset(\"rajpurkar/squad\")\n",
        "\n",
        "dataset_sq_train = dataset_sq[\"validation\"]\n",
        "\n",
        "df_sq = pd.DataFrame(\n",
        "    list(\n",
        "        zip(\n",
        "            dataset_sq_train[\"context\"],\n",
        "            dataset_sq_train[\"question\"],\n",
        "            dataset_sq_train[\"answers\"],\n",
        "        )\n",
        "    ),\n",
        "    columns=[\"context\", \"question\", \"answers\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sq = df_sq.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sq[\"answers\"][0][\"text\"] # ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
        "df_sq[\"answers\"] = df_sq[\"answers\"].apply(lambda x: x[\"text\"]) # ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
        "df_sq['answers'] = df_sq['answers'].apply(lambda x: list(set(x))) # ['Denver Broncos']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_sq\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Which NFL team represented the AFC at Super Bowl 50?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_sq"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ac40bafc-5613-4ab8-924a-2e7311506a42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>[Denver Broncos]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>[Carolina Panthers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>[Levi's Stadium, Santa Clara, California, Levi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac40bafc-5613-4ab8-924a-2e7311506a42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac40bafc-5613-4ab8-924a-2e7311506a42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac40bafc-5613-4ab8-924a-2e7311506a42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e69013e3-503e-45c3-ac6f-363b148bfbc6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e69013e3-503e-45c3-ac6f-363b148bfbc6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e69013e3-503e-45c3-ac6f-363b148bfbc6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             context  \\\n",
              "0  Super Bowl 50 was an American football game to...   \n",
              "1  Super Bowl 50 was an American football game to...   \n",
              "2  Super Bowl 50 was an American football game to...   \n",
              "\n",
              "                                            question  \\\n",
              "0  Which NFL team represented the AFC at Super Bo...   \n",
              "1  Which NFL team represented the NFC at Super Bo...   \n",
              "2                Where did Super Bowl 50 take place?   \n",
              "\n",
              "                                             answers  \n",
              "0                                   [Denver Broncos]  \n",
              "1                                [Carolina Panthers]  \n",
              "2  [Levi's Stadium, Santa Clara, California, Levi...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_sq.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
              " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
              " 'answers': ['Denver Broncos']}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Dictionary with questions and answers where index is the key\n",
        "sq_dict = df_sq.to_dict(orient=\"index\")\n",
        "sq_dict[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assistant_answers = []\n",
        "\n",
        "for index in range(len(sq_dict)):\n",
        "# for index in range(5):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Here is provided context for a question\" + sq_dict[index][\"question\"],\n",
        "        },\n",
        "\n",
        "        {   \"role\": \"user\",\n",
        "            \"content\": sq_dict[index][\"question\"]\n",
        "        },\n",
        "\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Based on the context of the input, answer the question\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = chat_with_model(messages, model, tokenizer, terminators)\n",
        "    assistant_answers.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sq[\"assistant_answer\"] = assistant_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_sq\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Which NFL team represented the AFC at Super Bowl 50?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"assistant_answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"The Denver Broncos represented the AFC at Super Bowl 50.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_sq"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1dba403f-2432-44f7-bea1-9cd787e7fcae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "      <th>assistant_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>[Denver Broncos]</td>\n",
              "      <td>The Denver Broncos represented the AFC at Supe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>[Carolina Panthers]</td>\n",
              "      <td>The Carolina Panthers represented the NFC at S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>[Levi's Stadium, Santa Clara, California, Levi...</td>\n",
              "      <td>Super Bowl 50 took place at Levi's Stadium in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>[Denver Broncos]</td>\n",
              "      <td>The Denver Broncos won Super Bowl 50.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>[gold]</td>\n",
              "      <td>The color used to emphasize the 50th anniversa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dba403f-2432-44f7-bea1-9cd787e7fcae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1dba403f-2432-44f7-bea1-9cd787e7fcae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1dba403f-2432-44f7-bea1-9cd787e7fcae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbca50d9-5194-4cb6-88d2-d90a89451c6c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbca50d9-5194-4cb6-88d2-d90a89451c6c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbca50d9-5194-4cb6-88d2-d90a89451c6c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             context  \\\n",
              "0  Super Bowl 50 was an American football game to...   \n",
              "1  Super Bowl 50 was an American football game to...   \n",
              "2  Super Bowl 50 was an American football game to...   \n",
              "3  Super Bowl 50 was an American football game to...   \n",
              "4  Super Bowl 50 was an American football game to...   \n",
              "\n",
              "                                            question  \\\n",
              "0  Which NFL team represented the AFC at Super Bo...   \n",
              "1  Which NFL team represented the NFC at Super Bo...   \n",
              "2                Where did Super Bowl 50 take place?   \n",
              "3                  Which NFL team won Super Bowl 50?   \n",
              "4  What color was used to emphasize the 50th anni...   \n",
              "\n",
              "                                             answers  \\\n",
              "0                                   [Denver Broncos]   \n",
              "1                                [Carolina Panthers]   \n",
              "2  [Levi's Stadium, Santa Clara, California, Levi...   \n",
              "3                                   [Denver Broncos]   \n",
              "4                                             [gold]   \n",
              "\n",
              "                                    assistant_answer  \n",
              "0  The Denver Broncos represented the AFC at Supe...  \n",
              "1  The Carolina Panthers represented the NFC at S...  \n",
              "2  Super Bowl 50 took place at Levi's Stadium in ...  \n",
              "3              The Denver Broncos won Super Bowl 50.  \n",
              "4  The color used to emphasize the 50th anniversa...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_sq.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BLEU Score\n",
        "\n",
        "The BLEU (Bilingual Evaluation Understudy) Score is a method for evaluating the quality of text which has been machine translated from one language to another. It is designed to measure the correspondence between a machine's output and that of a human.\n",
        "\n",
        "The formula for BLEU is as follows:\n",
        "\n",
        "$$\n",
        "\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^N w_n \\log p_n\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $p_n$ is the precision of n-grams,\n",
        "- $w_n$ are weights summing to 1,\n",
        "- $BP$ (Brevity Penalty) addresses the translation's length.\n",
        "\n",
        "The Brevity Penalty is calculated as:\n",
        "\n",
        "$$\n",
        "BP = \n",
        "\\begin{cases} \n",
        "1 & \\text{if } c > r \\\\\n",
        "e^{(1-r/c)} & \\text{if } c \\leq r\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- $c$ is the length of the candidate translation,\n",
        "- $r$ is the effective reference corpus length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/bleu/bleu.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: {'bleu': 0.023530115923927893, 'precisions': [0.3544973544973545, 0.10991957104557641, 0.0625, 0.03581267217630854], 'brevity_penalty': 0.24348537187522873, 'length_ratio': 0.4144736842105263, 'translation_length': 378, 'reference_length': 912}\n"
          ]
        }
      ],
      "source": [
        "# Extract predictions and references from DataFrame\n",
        "predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
        "references = list(df_qa[\"human_answer\"].astype(str))\n",
        "\n",
        "# Tokenize predictions\n",
        "tokenized_predictions = [pred.split() for pred in predictions]\n",
        "\n",
        "# Tokenize and wrap each reference in a list\n",
        "tokenized_references = [\n",
        "    [ref.split()] for ref in references\n",
        "]  # Wrap each tokenized reference in another list\n",
        "\n",
        "# Load BLEU metric\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_score = bleu.compute(\n",
        "    predictions=tokenized_predictions, references=tokenized_references\n",
        ")\n",
        "\n",
        "# Print the BLEU score\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ROUGE Score\n",
        "\n",
        "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score is used to evaluate automatic summarization and machine translation software. It compares an automatically produced summary or translation against a set of reference summaries, typically human-generated.\n",
        "\n",
        "The formula for the most commonly used variant, ROUGE-N, is as follows:\n",
        "\n",
        "$$\n",
        "\\text{ROUGE-N} = \\frac{{\\sum_{s \\in \\{Reference Summaries\\}} \\sum_{gram_n \\in s} Count_{match}(gram_n)}}{{\\sum_{s \\in \\{Reference Summaries\\}} \\sum_{gram_n \\in s} Count(gram_n)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $gram_n$ represents n-grams,\n",
        "- $Count_{match}(gram_n)$ is the count of n-grams in both the candidate summary and the reference summary,\n",
        "- $Count(gram_n)$ is the count of n-grams in the reference summary.\n",
        "\n",
        "This formula essentially calculates the proportion of n-grams in the reference summaries that are also found in the candidate summary, highlighting the importance of recall in the evaluation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': AggregateScore(low=Score(precision=0.2217391304347826, recall=0.08405608986124366, fmeasure=0.12427449542350297), mid=Score(precision=0.361402789171452, recall=0.1700527714365899, fmeasure=0.2194672748478193), high=Score(precision=0.5188101965788594, recall=0.26283544522983493, fmeasure=0.32007785545094486)), 'rouge2': AggregateScore(low=Score(precision=0.01557422969187675, recall=0.010714285714285714, fmeasure=0.011059907834101382), mid=Score(precision=0.09585434173669469, recall=0.03556997508305648, fmeasure=0.0485201950586631), high=Score(precision=0.21764705882352944, recall=0.06209302325581395, fmeasure=0.09566714061213799)), 'rougeL': AggregateScore(low=Score(precision=0.14293549802544975, recall=0.06174679348609311, fmeasure=0.08422798246183771), mid=Score(precision=0.24632743900586396, recall=0.10198990585877635, fmeasure=0.1377853334070059), high=Score(precision=0.40044283413848636, recall=0.13894402082830867, fmeasure=0.1894514370548604)), 'rougeLsum': AggregateScore(low=Score(precision=0.14056603773584903, recall=0.06281946993612804, fmeasure=0.08611922975915148), mid=Score(precision=0.24632743900586393, recall=0.10238396162768601, fmeasure=0.13778533340700594), high=Score(precision=0.40044283413848636, recall=0.13933807659721836, fmeasure=0.1894514370548604))}\n"
          ]
        }
      ],
      "source": [
        "# Adjust references for ROUGE\n",
        "rouge_references = [\n",
        "    ref[0] for ref in tokenized_references\n",
        "]\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "# Compute ROUGE score\n",
        "rouge_score = rouge.compute(\n",
        "    predictions=tokenized_predictions, references=rouge_references\n",
        ")\n",
        "\n",
        "# Print the ROUGE score\n",
        "print(rouge_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### METEOR Score\n",
        "\n",
        "The METEOR (Metric for Evaluation of Translation with Explicit ORdering) Score is a metric used to evaluate the quality of translations in natural language processing. Unlike BLEU, METEOR accounts for synonymy and stemming, and it aims to align words between the translated and reference texts more effectively.\n",
        "\n",
        "The basic formula for METEOR is as follows:\n",
        "\n",
        "$$\n",
        "\\text{METEOR} = (1 - Pen) \\cdot F_{mean}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $F_{mean}$ is the harmonic mean of precision and recall,\n",
        "- $Pen$ is a penalty for word order differences.\n",
        "\n",
        "The harmonic mean, $F_{mean}$, is calculated as:\n",
        "\n",
        "$$\n",
        "F_{mean} = \\frac{10 \\cdot P \\cdot R}{R + 9 \\cdot P}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $P$ is precision (the percentage of words in the translation that are correct),\n",
        "- $R$ is recall (the percentage of words in the reference that appear in the translation).\n",
        "\n",
        "The penalty, $Pen$, is calculated based on the number of chunks (contiguous non-matching phrases) in the alignment:\n",
        "\n",
        "$$\n",
        "Pen = 0.5 \\cdot \\left(\\frac{\\text{number of chunks}}{\\text{number of unigrams in the hypothesis}}\\right)^3\n",
        "$$\n",
        "\n",
        "This metric thus combines the lexical accuracy and structural similarity into a single score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/meteor/meteor.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'meteor': 0.13721729736237337}\n"
          ]
        }
      ],
      "source": [
        "joined_predictions = [\" \".join(pred) for pred in tokenized_predictions]\n",
        "joined_references = [\n",
        "    \" \".join(ref[0]) for ref in tokenized_references\n",
        "]  # Assuming only one reference per prediction and removing one level of list nesting\n",
        "\n",
        "# Load METEOR metric\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Compute METEOR score\n",
        "meteor_score = meteor.compute(\n",
        "    predictions=joined_predictions, references=joined_references\n",
        ")\n",
        "\n",
        "# Print the METEOR score\n",
        "print(meteor_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity: 0.9405923\n"
          ]
        }
      ],
      "source": [
        "# calculate the similarity between the embeddings of predicted and true sentences\n",
        "def similarity(sentences_prediction, sentences_true, model):\n",
        "\n",
        "    # Tokenization\n",
        "    tokens_prediction = [sentence.split() for sentence in sentences_prediction]\n",
        "    tokens_true = [sentence.split() for sentence in sentences_true]\n",
        "\n",
        "    embedding_prediction = [np.mean([model.wv[word] for word in tokens_prediction[i] if word in model.wv], axis=0) for i in range(len(tokens_prediction))]\n",
        "    embedding_true = [np.mean([model.wv[word] for word in tokens_true[i] if word in model.wv], axis=0) for i in range(len(tokens_true))]\n",
        "\n",
        "    embedding_prediction = np.array(embedding_prediction)\n",
        "    embedding_true = np.array(embedding_true)\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(embedding_prediction)):\n",
        "        similarity = np.dot(embedding_prediction[i], embedding_true[i]) / (np.linalg.norm(embedding_prediction[i]) * np.linalg.norm(embedding_true[i]))\n",
        "\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    return np.mean(similarities)\n",
        "\n",
        "sentences_prediction = [\"Paris is the capital of France.\", \"The cat is on the mat.\"]\n",
        "sentences_true = [\"Paris is not the capital of Italy.\", \"The cat is on the mat.\"]\n",
        "\n",
        "distanza = similarity(sentences_prediction, sentences_true, model_assistant)\n",
        "print(\"Similarity:\", distanza)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02e84784f9764550b000933dbc70c558": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d2090f14cf46878ed8daeec54af132": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cb82cd000a34ecaa4a24b7639bedc80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877812f447fa426994de88b237dafd80",
              "IPY_MODEL_e19c27f417694f15a69b2b6987d0a471",
              "IPY_MODEL_5450df6b1e2448d4a587fa828f9b9d5c"
            ],
            "layout": "IPY_MODEL_3f0e24d7f61c428dac396387bd9b8e1a"
          }
        },
        "13cb26101a5746e19cfd0085ac811c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20742a5208f84357ad523caeab7bce11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55c535f122dc47498ffad9dab206dced",
            "placeholder": "​",
            "style": "IPY_MODEL_5dbbf8c8285b4aefa8b8341a034298f5",
            "value": " 395/395 [00:00&lt;00:00, 17.3kB/s]"
          }
        },
        "2441716ab068433dba54017a5d53c3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24ef5fc62f4f49398f60aa54b79b473b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_290379391e8a4488a637a759ef120052",
              "IPY_MODEL_d497ff17d92243b39eed6b178f63d8f6",
              "IPY_MODEL_b021ef90c2064ff4841c174a5321ed3d"
            ],
            "layout": "IPY_MODEL_d8803f3537ab42a4809cfca46ad890cd"
          }
        },
        "25028418d18d40e289dc7abf01fedc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "290379391e8a4488a637a759ef120052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cb26101a5746e19cfd0085ac811c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_2441716ab068433dba54017a5d53c3af",
            "value": "Generating test split: 100%"
          }
        },
        "34352f8edc734177a4dd5aa65e923f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368b293ca59f4a769b2709621d7ae2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71c089c0388f4cd5999dd2cd3e80ec27",
            "max": 20877686,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4b978df62cd49dca6f901269fc2440c",
            "value": 20877686
          }
        },
        "3d3fc9747b1e4c7690ab770a9243ec73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0e24d7f61c428dac396387bd9b8e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b4cdf77ba84c62bdd65059a70372e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6754f496904d22860be6525f77e6b1",
            "placeholder": "​",
            "style": "IPY_MODEL_f00bd38fd36b48fd962f0ca3c6666675",
            "value": "Downloading data: 100%"
          }
        },
        "48e98e43a19842c19726a5a78008597d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "530210ca296d477c8c75aed4536ac1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5450df6b1e2448d4a587fa828f9b9d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b78a508ef3a440dc9ab715095dabbf1a",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f7e29d03c74ff8a1dbd0eaa4be2b4a",
            "value": " 9846/9846 [00:00&lt;00:00, 28370.86 examples/s]"
          }
        },
        "55c535f122dc47498ffad9dab206dced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbbf8c8285b4aefa8b8341a034298f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669dd15ce9d84be0b52690ae1c939ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71c089c0388f4cd5999dd2cd3e80ec27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "778757ae1d834c8381c0248a6a353e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77950b7b37f144549315a8bdb763b603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e86c22e69054684bb882f2c8d0cab2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f19b6f44b88420fb796d821b7aa2ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7d8a51e0a004acfacada7db8c0330ae",
            "placeholder": "​",
            "style": "IPY_MODEL_48e98e43a19842c19726a5a78008597d",
            "value": "Downloading data: 100%"
          }
        },
        "877812f447fa426994de88b237dafd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1731f17a6be460b9b2d436eba6ea5d9",
            "placeholder": "​",
            "style": "IPY_MODEL_e53bed3fc01b42b6b3593d53bed78eb2",
            "value": "Generating train split: 100%"
          }
        },
        "8c73089943ae449784a89d444e9b7f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb99ed37249e4dd7b065cda8fc371551",
            "max": 1105272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a196791aa71f4645a2f15d096142f6ef",
            "value": 1105272
          }
        },
        "8f047cc89e32422dbde213f54743f401": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a196791aa71f4645a2f15d096142f6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f7e29d03c74ff8a1dbd0eaa4be2b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9b3b72a45c44cd59ebd4889426ae53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c90b9a31c84c79a3897d141aab6227",
            "placeholder": "​",
            "style": "IPY_MODEL_8f047cc89e32422dbde213f54743f401",
            "value": " 1.11M/1.11M [00:00&lt;00:00, 3.47MB/s]"
          }
        },
        "b021ef90c2064ff4841c174a5321ed3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e94d3cef53b14c54a4e4be57fc75aa4d",
            "placeholder": "​",
            "style": "IPY_MODEL_530210ca296d477c8c75aed4536ac1af",
            "value": " 518/518 [00:00&lt;00:00, 14313.99 examples/s]"
          }
        },
        "b1731f17a6be460b9b2d436eba6ea5d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b78a508ef3a440dc9ab715095dabbf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d8a51e0a004acfacada7db8c0330ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf24977c534e4eb5b275dc9020a285b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47b4cdf77ba84c62bdd65059a70372e4",
              "IPY_MODEL_8c73089943ae449784a89d444e9b7f1e",
              "IPY_MODEL_a9b3b72a45c44cd59ebd4889426ae53a"
            ],
            "layout": "IPY_MODEL_02e84784f9764550b000933dbc70c558"
          }
        },
        "c7957c6388184c67aa30a8e47b13dc80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce6754f496904d22860be6525f77e6b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d497ff17d92243b39eed6b178f63d8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f888d236be5344d1aa278505be9489a5",
            "max": 518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7957c6388184c67aa30a8e47b13dc80",
            "value": 518
          }
        },
        "d4c90b9a31c84c79a3897d141aab6227": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8803f3537ab42a4809cfca46ad890cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da743f96a8644088b9cc3e148f00ae42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc42789b89b94cc1b84f2a6569c769fd",
              "IPY_MODEL_dffd6a7e61fe412caa6c46712f75f417",
              "IPY_MODEL_20742a5208f84357ad523caeab7bce11"
            ],
            "layout": "IPY_MODEL_08d2090f14cf46878ed8daeec54af132"
          }
        },
        "dc42789b89b94cc1b84f2a6569c769fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669dd15ce9d84be0b52690ae1c939ee1",
            "placeholder": "​",
            "style": "IPY_MODEL_778757ae1d834c8381c0248a6a353e6e",
            "value": "Downloading readme: 100%"
          }
        },
        "dffd6a7e61fe412caa6c46712f75f417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e86c22e69054684bb882f2c8d0cab2b",
            "max": 395,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25028418d18d40e289dc7abf01fedc99",
            "value": 395
          }
        },
        "e19c27f417694f15a69b2b6987d0a471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3fc9747b1e4c7690ab770a9243ec73",
            "max": 9846,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77950b7b37f144549315a8bdb763b603",
            "value": 9846
          }
        },
        "e4b978df62cd49dca6f901269fc2440c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e501ec8bfb59446babe057633ef46040": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34352f8edc734177a4dd5aa65e923f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_fd6e9354adff4702a5d3dce8a0d1c96c",
            "value": " 20.9M/20.9M [00:00&lt;00:00, 24.0MB/s]"
          }
        },
        "e53bed3fc01b42b6b3593d53bed78eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e76d3770a00640dd8703b360dae1d88c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94d3cef53b14c54a4e4be57fc75aa4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00bd38fd36b48fd962f0ca3c6666675": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f888d236be5344d1aa278505be9489a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88f8a0035a84ad7a54ca2d7de19ede4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f19b6f44b88420fb796d821b7aa2ea1",
              "IPY_MODEL_368b293ca59f4a769b2709621d7ae2a0",
              "IPY_MODEL_e501ec8bfb59446babe057633ef46040"
            ],
            "layout": "IPY_MODEL_e76d3770a00640dd8703b360dae1d88c"
          }
        },
        "fb99ed37249e4dd7b065cda8fc371551": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6e9354adff4702a5d3dce8a0d1c96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
