{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install gradio\n",
        "!pip install langid\n",
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install noisereduce\n",
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "8gS6XcUZEMzG",
        "outputId": "a8d818d7-74bb-474a-ecec-e01e3cdefba1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-123c6c9ff6fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install emoji'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gradio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install langid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install nltk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHatw0zXD5ho"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "oKKdTOwVD5hw",
        "outputId": "014d7e30-766d-47c1-cb5c-82eb3a39a095"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'emoji'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b92dc588d96a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import textwrap\n",
        "import time\n",
        "import uuid\n",
        "import wave\n",
        "\n",
        "import emoji\n",
        "import gradio as gr\n",
        "import langid\n",
        "import nltk\n",
        "import numpy as np\n",
        "import noisereduce as nr\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Download the 'punkt' tokenizer for the NLTK library\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# will use api to restart space on a unrecoverable error\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "REPO_ID = os.environ.get(\"REPO_ID\")\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "latent_map = {}\n",
        "\n",
        "def get_latents(chatbot_voice, xtts_model, voice_cleanup=False):\n",
        "    global latent_map\n",
        "    if chatbot_voice not in latent_map:\n",
        "        speaker_wav = f\"examples/{chatbot_voice}.wav\"\n",
        "        if (voice_cleanup):\n",
        "            try:\n",
        "                cleanup_filter=\"lowpass=8000,highpass=75,areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02,areverse,silenceremove=start_periods=1:start_silence=0:start_threshold=0.02\"\n",
        "                resample_filter=\"-ac 1 -ar 22050\"\n",
        "                out_filename = speaker_wav + str(uuid.uuid4()) + \".wav\"  #ffmpeg to know output format\n",
        "                #we will use newer ffmpeg as that has afftn denoise filter\n",
        "                shell_command = f\"ffmpeg -y -i {speaker_wav} -af {cleanup_filter} {resample_filter} {out_filename}\".split(\" \")\n",
        "                command_result = subprocess.run([item for item in shell_command], capture_output=False,text=True, check=True)\n",
        "                speaker_wav=out_filename\n",
        "                print(\"Filtered microphone input\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                # There was an error - command exited with non-zero code\n",
        "                print(\"Error: failed filtering, use original microphone input\")\n",
        "        else:\n",
        "                speaker_wav=speaker_wav\n",
        "        # gets condition latents from the model\n",
        "        # returns tuple (gpt_cond_latent, speaker_embedding)\n",
        "        latent_map[chatbot_voice] = xtts_model.get_conditioning_latents(audio_path=speaker_wav)\n",
        "    return latent_map[chatbot_voice]\n",
        "\n",
        "\n",
        "def detect_language(prompt, xtts_supported_languages=None):\n",
        "    if xtts_supported_languages is None:\n",
        "        xtts_supported_languages = [\"en\",\"es\",\"fr\",\"de\",\"it\",\"pt\",\"pl\",\"tr\",\"ru\",\"nl\",\"cs\",\"ar\",\"zh-cn\",\"ja\"]\n",
        "\n",
        "    # Fast language autodetection\n",
        "    if len(prompt)>15:\n",
        "        language_predicted=langid.classify(prompt)[0].strip() # strip need as there is space at end!\n",
        "        if language_predicted == \"zh\":\n",
        "            #we use zh-cn on xtts\n",
        "            language_predicted = \"zh-cn\"\n",
        "\n",
        "        if language_predicted not in xtts_supported_languages:\n",
        "            print(f\"Detected a language not supported by xtts :{language_predicted}, switching to english for now\")\n",
        "            gr.Warning(f\"Language detected '{language_predicted}' can not be spoken properly 'yet' \")\n",
        "            language= \"en\"\n",
        "        else:\n",
        "            language = language_predicted\n",
        "        print(f\"Language: Predicted sentence language:{language_predicted} , using language for xtts:{language}\")\n",
        "    else:\n",
        "        # Hard to detect language fast in short sentence, use english default\n",
        "        language = \"en\"\n",
        "        print(f\"Language: Prompt is short or autodetect language disabled using english for xtts\")\n",
        "\n",
        "    return language\n",
        "\n",
        "def get_voice_streaming(prompt, language, chatbot_voice, xtts_model, suffix=\"0\"):\n",
        "    gpt_cond_latent, speaker_embedding = get_latents(chatbot_voice, xtts_model)\n",
        "    try:\n",
        "        t0 = time.time()\n",
        "        chunks = xtts_model.inference_stream(\n",
        "            prompt,\n",
        "            language,\n",
        "            gpt_cond_latent,\n",
        "            speaker_embedding,\n",
        "            repetition_penalty=7.0,\n",
        "            temperature=0.85,\n",
        "        )\n",
        "\n",
        "        first_chunk = True\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if first_chunk:\n",
        "                first_chunk_time = time.time() - t0\n",
        "                metrics_text = f\"Latency to first audio chunk: {round(first_chunk_time*1000)} milliseconds\\n\"\n",
        "                first_chunk = False\n",
        "            #print(f\"Received chunk {i} of audio length {chunk.shape[-1]}\")\n",
        "\n",
        "            # In case output is required to be multiple voice files\n",
        "            # out_file = f'{char}_{i}.wav'\n",
        "            # write(out_file, 24000, chunk.detach().cpu().numpy().squeeze())\n",
        "            # audio = AudioSegment.from_file(out_file)\n",
        "            # audio.export(out_file, format='wav')\n",
        "            # return out_file\n",
        "            # directly return chunk as bytes for streaming\n",
        "            chunk = chunk.detach().cpu().numpy().squeeze()\n",
        "            chunk = (chunk * 32767).astype(np.int16)\n",
        "            yield chunk.tobytes()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"device-side assert\" in str(e):\n",
        "            # cannot do anything on cuda device side error, need tor estart\n",
        "            print(\n",
        "                f\"Exit due to: Unrecoverable exception caused by prompt:{prompt}\",\n",
        "                flush=True,\n",
        "            )\n",
        "            gr.Warning(\"Unhandled Exception encounter, please retry in a minute\")\n",
        "            print(\"Cuda device-assert Runtime encountered need restart\")\n",
        "\n",
        "            # HF Space specific.. This error is unrecoverable need to restart space\n",
        "            api.restart_space(REPO_ID=REPO_ID)\n",
        "        else:\n",
        "            print(\"RuntimeError: non device-side assert error:\", str(e))\n",
        "            # Does not require warning happens on empty chunk and at end\n",
        "            ###gr.Warning(\"Unhandled Exception encounter, please retry in a minute\")\n",
        "            return None\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def wave_header_chunk(frame_input=b\"\", channels=1, sample_width=2, sample_rate=24000):\n",
        "    # This will create a wave header then append the frame input\n",
        "    # It should be first on a streaming wav file\n",
        "    # Other frames better should not have it (else you will hear some artifacts each chunk start)\n",
        "    wav_buf = io.BytesIO()\n",
        "    with wave.open(wav_buf, \"wb\") as vfout:\n",
        "        vfout.setnchannels(channels)\n",
        "        vfout.setsampwidth(sample_width)\n",
        "        vfout.setframerate(sample_rate)\n",
        "        vfout.writeframes(frame_input)\n",
        "\n",
        "    wav_buf.seek(0)\n",
        "    return wav_buf.read()\n",
        "\n",
        "def format_prompt(message, history):\n",
        "    system_message = f\"\"\"\n",
        "    You are an empathetic, insightful, and supportive coach who helps people deal with challenges and celebrate achievements.\n",
        "    You help people feel better by asking questions to reflect on and evoke feelings of positivity, gratitude, joy, and love.\n",
        "    You show radical candor and tough love.\n",
        "    Respond in a casual and friendly tone.\n",
        "    Sprinkle in filler words, contractions, idioms, and other casual speech that we use in conversation.\n",
        "    Emulate the user's speaking style and be concise in your response.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"<s>[INST]\" + system_message + \"[/INST]\"\n",
        "    )\n",
        "    for user_prompt, bot_response in history:\n",
        "        if user_prompt is not None:\n",
        "            prompt += f\"[INST] {user_prompt} [/INST]\"\n",
        "        prompt += f\" {bot_response}</s> \"\n",
        "\n",
        "    if message==\"\":\n",
        "        message=\"Hello\"\n",
        "    prompt += f\"[INST] {message} [/INST]\"\n",
        "    return prompt\n",
        "\n",
        "def generate_llm_output(\n",
        "        prompt,\n",
        "        history,\n",
        "        llm,\n",
        "        temperature=0.8,\n",
        "        max_tokens=256,\n",
        "        top_p=0.95,\n",
        "        stop_words=[\"<s>\",\"[/INST]\", \"</s>\"]\n",
        "    ):\n",
        "        temperature = float(temperature)\n",
        "        if temperature < 1e-2:\n",
        "            temperature = 1e-2\n",
        "        top_p = float(top_p)\n",
        "\n",
        "        generate_kwargs = dict(\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=top_p,\n",
        "            stop=stop_words\n",
        "        )\n",
        "        formatted_prompt = format_prompt(prompt, history)\n",
        "        try:\n",
        "            print(\"LLM Input:\", formatted_prompt)\n",
        "            # Local GGUF\n",
        "            stream = llm(\n",
        "                formatted_prompt,\n",
        "                **generate_kwargs,\n",
        "                stream=True,\n",
        "            )\n",
        "            output = \"\"\n",
        "            for response in stream:\n",
        "                character= response[\"choices\"][0][\"text\"]\n",
        "\n",
        "                if character in stop_words:\n",
        "                    # end of context\n",
        "                    return\n",
        "\n",
        "                if emoji.is_emoji(character):\n",
        "                    # Bad emoji not a meaning messes chat from next lines\n",
        "                    return\n",
        "\n",
        "                output += response[\"choices\"][0][\"text\"]\n",
        "                yield output\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Unhandled Exception: \", str(e))\n",
        "            gr.Warning(\"Unfortunately Mistral is unable to process\")\n",
        "            output = \"I do not know what happened but I could not understand you .\"\n",
        "        return output\n",
        "\n",
        "def get_sentence(history, llm):\n",
        "    history = [[\"\", None]] if history is None else history\n",
        "    history[-1][1] = \"\"\n",
        "    sentence_list = []\n",
        "    sentence_hash_list = []\n",
        "\n",
        "    text_to_generate = \"\"\n",
        "    stored_sentence = None\n",
        "    stored_sentence_hash = None\n",
        "\n",
        "    for character in generate_llm_output(history[-1][0], history[:-1], llm):\n",
        "        history[-1][1] = character.replace(\"<|assistant|>\",\"\")\n",
        "        # It is coming word by word\n",
        "        text_to_generate = nltk.sent_tokenize(history[-1][1].replace(\"\\n\", \" \").replace(\"<|assistant|>\",\" \").replace(\"<|ass>\",\"\").replace(\"[/ASST]\",\"\").replace(\"[/ASSI]\",\"\").replace(\"[/ASS]\",\"\").replace(\"\u0011\",\"\").strip())\n",
        "        if len(text_to_generate) > 1:\n",
        "\n",
        "            dif = len(text_to_generate) - len(sentence_list)\n",
        "\n",
        "            if dif == 1 and len(sentence_list) != 0:\n",
        "                continue\n",
        "\n",
        "            if dif == 2 and len(sentence_list) != 0 and stored_sentence is not None:\n",
        "                continue\n",
        "\n",
        "            # All this complexity due to trying append first short sentence to next one for proper language auto-detect\n",
        "            if stored_sentence is not None and stored_sentence_hash is None and dif>1:\n",
        "                #means we consumed stored sentence and should look at next sentence to generate\n",
        "                sentence = text_to_generate[len(sentence_list)+1]\n",
        "            elif stored_sentence is not None and len(text_to_generate)>2 and stored_sentence_hash is not None:\n",
        "                print(\"Appending stored\")\n",
        "                sentence = stored_sentence + text_to_generate[len(sentence_list)+1]\n",
        "                stored_sentence_hash = None\n",
        "            else:\n",
        "                sentence = text_to_generate[len(sentence_list)]\n",
        "\n",
        "            # too short sentence just append to next one if there is any\n",
        "            # this is for proper language detection\n",
        "            if len(sentence)<=15 and stored_sentence_hash is None and stored_sentence is None:\n",
        "                if sentence[-1] in [\".\",\"!\",\"?\"]:\n",
        "                    if stored_sentence_hash != hash(sentence):\n",
        "                        stored_sentence = sentence\n",
        "                        stored_sentence_hash = hash(sentence)\n",
        "                        print(\"Storing:\",stored_sentence)\n",
        "                        continue\n",
        "\n",
        "\n",
        "            sentence_hash = hash(sentence)\n",
        "            if stored_sentence_hash is not None and sentence_hash == stored_sentence_hash:\n",
        "                continue\n",
        "\n",
        "            if sentence_hash not in sentence_hash_list:\n",
        "                sentence_hash_list.append(sentence_hash)\n",
        "                sentence_list.append(sentence)\n",
        "                print(\"New Sentence: \", sentence)\n",
        "                yield (sentence, history)\n",
        "\n",
        "    # return that final sentence token\n",
        "    try:\n",
        "        last_sentence = nltk.sent_tokenize(history[-1][1].replace(\"\\n\", \" \").replace(\"<|ass>\",\"\").replace(\"[/ASST]\",\"\").replace(\"[/ASSI]\",\"\").replace(\"[/ASS]\",\"\").replace(\"\u0011\",\"\").strip())[-1]\n",
        "        sentence_hash = hash(last_sentence)\n",
        "        if sentence_hash not in sentence_hash_list:\n",
        "            if stored_sentence is not None and stored_sentence_hash is not None:\n",
        "                last_sentence = stored_sentence + last_sentence\n",
        "                stored_sentence = stored_sentence_hash = None\n",
        "                print(\"Last Sentence with stored:\",last_sentence)\n",
        "\n",
        "            sentence_hash_list.append(sentence_hash)\n",
        "            sentence_list.append(last_sentence)\n",
        "            print(\"Last Sentence: \", last_sentence)\n",
        "\n",
        "            yield (last_sentence, history)\n",
        "    except:\n",
        "        print(\"ERROR on last sentence history is :\", history)\n",
        "\n",
        "# will generate speech audio file per sentence\n",
        "def generate_speech_for_sentence(history, chatbot_voice, sentence, xtts_model, xtts_supported_languages=None, filter_output=True, return_as_byte=False):\n",
        "    language = \"autodetect\"\n",
        "\n",
        "    wav_bytestream = b\"\"\n",
        "\n",
        "    if len(sentence)==0:\n",
        "        print(\"EMPTY SENTENCE\")\n",
        "        return\n",
        "\n",
        "    # Sometimes prompt </s> coming on output remove it\n",
        "    # Some post process for speech only\n",
        "    sentence = sentence.replace(\"</s>\", \"\")\n",
        "    # remove code from speech\n",
        "    sentence = re.sub(\"```.*```\", \"\", sentence, flags=re.DOTALL)\n",
        "    sentence = re.sub(\"`.*`\", \"\", sentence, flags=re.DOTALL)\n",
        "\n",
        "    sentence = re.sub(\"\\(.*\\)\", \"\", sentence, flags=re.DOTALL)\n",
        "\n",
        "    sentence = sentence.replace(\"```\", \"\")\n",
        "    sentence = sentence.replace(\"...\", \" \")\n",
        "    sentence = sentence.replace(\"(\", \" \")\n",
        "    sentence = sentence.replace(\")\", \" \")\n",
        "    sentence = sentence.replace(\"<|assistant|>\",\"\")\n",
        "\n",
        "    if len(sentence)==0:\n",
        "        print(\"EMPTY SENTENCE after processing\")\n",
        "        return\n",
        "\n",
        "    # A fast fix for last chacter, may produce weird sounds if it is with text\n",
        "    #if (sentence[-1] in [\"!\", \"?\", \".\", \",\"]) or (sentence[-2] in [\"!\", \"?\", \".\", \",\"]):\n",
        "    #    # just add a space\n",
        "    #    sentence = sentence[:-1] + \" \" + sentence[-1]\n",
        "\n",
        "    # regex does the job well\n",
        "    sentence= re.sub(\"([^\\x00-\\x7F]|\\w)(\\.|\\。|\\?|\\!)\",r\"\\1 \\2\\2\",sentence)\n",
        "\n",
        "    print(\"Sentence for speech:\", sentence)\n",
        "\n",
        "\n",
        "    try:\n",
        "        SENTENCE_SPLIT_LENGTH=350\n",
        "        if len(sentence)<SENTENCE_SPLIT_LENGTH:\n",
        "            # no problem continue on\n",
        "            sentence_list = [sentence]\n",
        "        else:\n",
        "            # Until now nltk likely split sentences properly but we need additional\n",
        "            # check for longer sentence and split at last possible position\n",
        "            # Do whatever necessary, first break at hypens then spaces and then even split very long words\n",
        "            sentence_list=textwrap.wrap(sentence,SENTENCE_SPLIT_LENGTH)\n",
        "            print(\"SPLITTED LONG SENTENCE:\",sentence_list)\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "\n",
        "            if any(c.isalnum() for c in sentence):\n",
        "                if language==\"autodetect\":\n",
        "                    #on first call autodetect, nexts sentence calls will use same language\n",
        "                    language = detect_language(sentence, xtts_supported_languages)\n",
        "\n",
        "                #exists at least 1 alphanumeric (utf-8)\n",
        "                audio_stream = get_voice_streaming(\n",
        "                        sentence, language, chatbot_voice, xtts_model\n",
        "                    )\n",
        "            else:\n",
        "                # likely got a ' or \" or some other text without alphanumeric in it\n",
        "                audio_stream = None\n",
        "\n",
        "            # XTTS is actually using streaming response but we are playing audio by sentence\n",
        "            # If you want direct XTTS voice streaming (send each chunk to voice ) you may set DIRECT_STREAM=1 environment variable\n",
        "            if audio_stream is not None:\n",
        "                frame_length = 0\n",
        "                for chunk in audio_stream:\n",
        "                    try:\n",
        "                        wav_bytestream += chunk\n",
        "                        frame_length += len(chunk)\n",
        "                    except:\n",
        "                        # hack to continue on playing. sometimes last chunk is empty , will be fixed on next TTS\n",
        "                        continue\n",
        "\n",
        "            # Filter output for better voice\n",
        "            if filter_output:\n",
        "                data_s16 = np.frombuffer(wav_bytestream, dtype=np.int16, count=len(wav_bytestream)//2, offset=0)\n",
        "                float_data = data_s16 * 0.5**15\n",
        "                reduced_noise = nr.reduce_noise(y=float_data, sr=24000,prop_decrease =0.8,n_fft=1024)\n",
        "                wav_bytestream = (reduced_noise * 32767).astype(np.int16)\n",
        "                wav_bytestream = wav_bytestream.tobytes()\n",
        "\n",
        "            if audio_stream is not None:\n",
        "                if not return_as_byte:\n",
        "                    audio_unique_filename = \"/tmp/\"+ str(uuid.uuid4())+\".wav\"\n",
        "                    with wave.open(audio_unique_filename, \"w\") as f:\n",
        "                        f.setnchannels(1)\n",
        "                        # 2 bytes per sample.\n",
        "                        f.setsampwidth(2)\n",
        "                        f.setframerate(24000)\n",
        "                        f.writeframes(wav_bytestream)\n",
        "\n",
        "                    return (history , gr.Audio.update(value=audio_unique_filename, autoplay=True))\n",
        "                else:\n",
        "                    return (history , gr.Audio.update(value=wav_bytestream, autoplay=True))\n",
        "    except RuntimeError as e:\n",
        "        if \"device-side assert\" in str(e):\n",
        "            # cannot do anything on cuda device side error, need tor estart\n",
        "            print(\n",
        "                f\"Exit due to: Unrecoverable exception caused by prompt:{sentence}\",\n",
        "                flush=True,\n",
        "            )\n",
        "            gr.Warning(\"Unhandled Exception encounter, please retry in a minute\")\n",
        "            print(\"Cuda device-assert Runtime encountered need restart\")\n",
        "\n",
        "            # HF Space specific.. This error is unrecoverable need to restart space\n",
        "            api.restart_space(REPO_ID=REPO_ID)\n",
        "        else:\n",
        "            print(\"RuntimeError: non device-side assert error:\", str(e))\n",
        "            raise e\n",
        "\n",
        "    print(\"All speech ended\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nGUbZg8D5h1"
      },
      "source": [
        "# Set up Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install TTS"
      ],
      "metadata": {
        "id": "xtxHCRwC2b87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"CUDACXX\"] = \"/usr/local/cuda/bin/nvcc\"\n",
        "# os.system(\"python -m unidic download\")\n",
        "# os.system(\n",
        "#     'CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.11 --verbose'\n",
        "# )"
      ],
      "metadata": {
        "id": "GCZ_gkSJ4_pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faster-whisper\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "EE4jV6yX5bGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Prg5wUZD5h2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from faster_whisper import WhisperModel\n",
        "import gradio as gr\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "from TTS.utils.generic_utils import get_user_data_dir\n",
        "from TTS.utils.manage import ModelManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmtujrp2D5h2"
      },
      "source": [
        "# Import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXbydxN4D5h3"
      },
      "outputs": [],
      "source": [
        "# Load Whisper model\n",
        "whisper_model = WhisperModel(\"large-v3\", device=\"cuda\", compute_type=\"float16\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -U flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "4SPlU5v29QiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "Xt-jfgm-9VUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install huggingface-hub\n",
        "\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "2SJLEaA99X31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "TuDei_qcMFO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFa4CNs5D5h3"
      },
      "outputs": [],
      "source": [
        "# Load Mistral LLM model\n",
        "# Load Mistral LLM model\n",
        "mistral_model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
        "    filename=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
        ")\n",
        "\n",
        "mistral_llm = Llama(\n",
        "    model_path=mistral_model_path,\n",
        "    n_gpu_layers=35,\n",
        "    max_new_tokens=256,\n",
        "    context_window=4096,\n",
        "    n_ctx=4096,\n",
        "    n_batch=128,\n",
        "    verbose=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "vDlX86pdOf3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = XttsConfig()\n",
        "config.load_json(\"./XTTS-v2/config.json\")\n",
        "xtts_model = Xtts.init_from_config(config)\n",
        "xtts_model.load_checkpoint(config, checkpoint_dir=\"./XTTS-v2/\")\n",
        "xtts_model.cuda()"
      ],
      "metadata": {
        "id": "zh7h805x_eOO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_speak = \"We are going to go to the gym now to lift heavy circles.\"\n",
        "reference_audios = [\"./female.wav\",\"./male.wav\"]\n",
        "\n",
        "outputs = xtts_model.synthesize(\n",
        "    text_to_speak,\n",
        "    config,\n",
        "    speaker_wav=reference_audios,\n",
        "    gpt_cond_len=3,\n",
        "    language=\"en\",\n",
        ")"
      ],
      "metadata": {
        "id": "uN0s9iZqUOBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "CplDfG5wUtIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(data=outputs['wav'], rate=24000)"
      ],
      "metadata": {
        "id": "S9X325DNUctn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDK-772eD5h5"
      },
      "source": [
        "# Set up Gradio Interface ######\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynwpb5VkD5h5"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Voice chat with LLM\") as demo:\n",
        "    DESCRIPTION = \"\"\"# Voice chat with LLM\"\"\"\n",
        "    gr.Markdown(DESCRIPTION)\n",
        "\n",
        "    # Define chatbot component\n",
        "    chatbot = gr.Chatbot(\n",
        "        value=[\n",
        "            (None, \"Hi friend, I'm Amy, an AI coach. How can I help you today?\")\n",
        "        ],  # Initial greeting from the chatbot\n",
        "        elem_id=\"chatbot\",\n",
        "        avatar_images=(\"examples/hf-logo.png\", \"examples/ai-chat-logo.png\"),\n",
        "        bubble_full_width=False,\n",
        "    )\n",
        "\n",
        "    # Define chatbot voice component\n",
        "    VOICES = [\"female\", \"male\"]\n",
        "    with gr.Row():\n",
        "        chatbot_voice = gr.Dropdown(\n",
        "            label=\"Voice of the Chatbot\",\n",
        "            info=\"How should Chatbot talk like\",\n",
        "            choices=VOICES,\n",
        "            max_choices=1,\n",
        "            value=VOICES[0],\n",
        "        )\n",
        "\n",
        "    # Define text and audio record input components\n",
        "    with gr.Row():\n",
        "        txt_box = gr.Textbox(\n",
        "            scale=3,\n",
        "            show_label=False,\n",
        "            placeholder=\"Enter text and press enter, or speak to your microphone\",\n",
        "            container=False,\n",
        "            interactive=True,\n",
        "        )\n",
        "        audio_record = gr.Audio(label=\"Upload Audio\", type=\"filepath\")\n",
        "\n",
        "    # Define generated audio playback component\n",
        "    with gr.Row():\n",
        "        sentence = gr.Textbox(visible=False)\n",
        "        audio_playback = gr.Audio(\n",
        "            value=None,\n",
        "            label=\"Generated audio response\",\n",
        "            streaming=True,\n",
        "            autoplay=True,\n",
        "            interactive=False,\n",
        "            show_label=True,\n",
        "        )\n",
        "\n",
        "    # Will be triggered on text submit (will send to generate_speech)\n",
        "    def add_text(chatbot_history, text):\n",
        "        chatbot_history = [] if chatbot_history is None else chatbot_history\n",
        "        chatbot_history = chatbot_history + [(text, None)]\n",
        "        return chatbot_history, gr.update(value=\"\", interactive=False)\n",
        "\n",
        "    # Will be triggered on voice submit (will transribe and send to generate_speech)\n",
        "    def add_audio(chatbot_history, audio):\n",
        "        chatbot_history = [] if chatbot_history is None else chatbot_history\n",
        "        # get result from whisper and strip it to delete begin and end space\n",
        "        response, _ = whisper_model.transcribe(audio)\n",
        "        text = list(response)[0].text.strip()\n",
        "        print(\"Transcribed text:\", text)\n",
        "        chatbot_history = chatbot_history + [(text, None)]\n",
        "        return chatbot_history, gr.update(value=\"\", interactive=False)\n",
        "\n",
        "    def generate_speech(chatbot_history, chatbot_voice, initial_greeting=False):\n",
        "        # Start by yielding an initial empty audio to set up autoplay\n",
        "        yield (\"\", chatbot_history, wave_header_chunk())\n",
        "\n",
        "        # Helper function to handle the speech generation and yielding process\n",
        "        def handle_speech_generation(sentence, chatbot_history, chatbot_voice):\n",
        "            if sentence != \"\":\n",
        "                print(\"Processing sentence\")\n",
        "                generated_speech = generate_speech_for_sentence(\n",
        "                    chatbot_history,\n",
        "                    chatbot_voice,\n",
        "                    sentence,\n",
        "                    xtts_model,\n",
        "                    xtts_supported_languages=config.languages,\n",
        "                    return_as_byte=True,\n",
        "                )\n",
        "                if generated_speech is not None:\n",
        "                    _, audio_dict = generated_speech\n",
        "                    yield (sentence, chatbot_history, audio_dict[\"value\"])\n",
        "\n",
        "        if initial_greeting:\n",
        "            # Process only the initial greeting if specified\n",
        "            for _, sentence in chatbot_history:\n",
        "                yield from handle_speech_generation(\n",
        "                    sentence, chatbot_history, chatbot_voice\n",
        "                )\n",
        "        else:\n",
        "            # Continuously get and process sentences from a generator function\n",
        "            for sentence, chatbot_history in get_sentence(chatbot_history, mistral_llm):\n",
        "                print(\"Inserting sentence to queue\")\n",
        "                yield from handle_speech_generation(\n",
        "                    sentence, chatbot_history, chatbot_voice\n",
        "                )\n",
        "\n",
        "    txt_msg = txt_box.submit(\n",
        "        fn=add_text, inputs=[chatbot, txt_box], outputs=[chatbot, txt_box], queue=False\n",
        "    ).then(\n",
        "        fn=generate_speech,\n",
        "        inputs=[chatbot, chatbot_voice],\n",
        "        outputs=[sentence, chatbot, audio_playback],\n",
        "    )\n",
        "\n",
        "    txt_msg.then(\n",
        "        fn=lambda: gr.update(interactive=True),\n",
        "        inputs=None,\n",
        "        outputs=[txt_box],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "    audio_msg = audio_record.stop_recording(\n",
        "        fn=add_audio,\n",
        "        inputs=[chatbot, audio_record],\n",
        "        outputs=[chatbot, txt_box],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=generate_speech,\n",
        "        inputs=[chatbot, chatbot_voice],\n",
        "        outputs=[sentence, chatbot, audio_playback],\n",
        "    )\n",
        "\n",
        "    audio_msg.then(\n",
        "        fn=lambda: (\n",
        "            gr.update(interactive=True),\n",
        "            gr.update(interactive=True, value=None),\n",
        "        ),\n",
        "        inputs=None,\n",
        "        outputs=[txt_box, audio_record],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "    FOOTNOTE = \"\"\"\n",
        "            This Space demonstrates how to speak to an llm chatbot, based solely on open accessible models.\n",
        "            It relies on the following models :\n",
        "            - Speech to Text Model: [Faster-Whisper-large-v3](https://huggingface.co/Systran/faster-whisper-large-v3) an ASR model, to transcribe recorded audio to text.\n",
        "            - Large Language Model: [Mistral-7b-instruct-v0.1-quantized](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) a LLM to generate the chatbot responses.\n",
        "            - Text to Speech Model: [XTTS-v2](https://huggingface.co/spaces/coqui/xtts) a TTS model, to generate the voice of the chatbot.\n",
        "\n",
        "            Note:\n",
        "            - Responses generated by chat model should not be assumed correct or taken serious, as this is a demonstration example only\n",
        "            - iOS (Iphone/Ipad) devices may not experience voice due to autoplay being disabled on these devices by Vendor\"\"\"\n",
        "    gr.Markdown(FOOTNOTE)\n",
        "    demo.load(\n",
        "        fn=generate_speech,\n",
        "        inputs=[chatbot, chatbot_voice, gr.State(value=True)],\n",
        "        outputs=[sentence, chatbot, audio_playback],\n",
        "    )\n",
        "\n",
        "demo.queue().launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nHatw0zXD5ho"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}